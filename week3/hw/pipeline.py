#!/usr/bin/env python3
"""
å®Œæ•´çš„PDF RAGæµæ°´çº¿å®ç°
åŒ…å«8ä¸ªæ ¸å¿ƒæ­¥éª¤ï¼šElasticsearchéƒ¨ç½²ã€PDFå¤„ç†ã€å†…å®¹åˆ‡åˆ†ã€å‘é‡åŒ–ã€ç´¢å¼•ã€æ£€ç´¢ã€é‡æ’åºã€å›ç­”ç”Ÿæˆ
æ”¯æŒå‘½ä»¤è¡Œå’Œäº¤äº’å¼ä½¿ç”¨
"""

import argparse
import glob
import os
import sys
import time
import traceback
from typing import Dict, List, Any, Optional

# å¯¼å…¥ç°æœ‰æ¨¡å—
from config import get_es, ElasticConfig
from document_process import process_pdf, num_tokens_from_string
from embedding import local_embedding
from es_functions import create_elastic_index, delete_elastic_index
from image_table import extract_images_from_pdf, extract_tables_from_pdf
from retrieve_documents import elastic_search, rerank, rag_fusion, coreference_resolution, query_decompositon
from websearch import bocha_web_search, ask_llm
from langchain.text_splitter import RecursiveCharacterTextSplitter
import fitz
import json


class RAGPipeline:
    """PDF RAGå®Œæ•´æµæ°´çº¿"""
    
    def __init__(self, index_name: str = "rag_pipeline_index"):
        """åˆå§‹åŒ–æµæ°´çº¿
        
        Args:
            index_name: Elasticsearchç´¢å¼•åç§°
        """
        self.index_name = index_name
        self.es = None
        self.chat_history = []
        self.processed_pdfs = []
        
    def step1_deploy_elasticsearch(self) -> Dict[str, Any]:
        """æ­¥éª¤1: åœ¨æœ¬åœ°éƒ¨ç½²Elasticsearch"""
        print("ğŸš€ æ­¥éª¤1: æ£€æŸ¥Elasticsearchéƒ¨ç½²...")
        
        try:
            self.es = get_es()
            
            # æµ‹è¯•è¿æ¥
            if self.es.ping():
                print("âœ… Elasticsearchè¿æ¥æˆåŠŸ")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ›å»ºç´¢å¼•
                if not self.es.indices.exists(index=self.index_name):
                    create_elastic_index(self.index_name)
                    print(f"âœ… ç´¢å¼• {self.index_name} åˆ›å»ºæˆåŠŸ")
                else:
                    print(f"âœ… ç´¢å¼• {self.index_name} å·²å­˜åœ¨")
                
                return {"success": True, "message": "Elasticsearchéƒ¨ç½²æ­£å¸¸"}
            else:
                return {"success": False, "error": "Elasticsearchè¿æ¥å¤±è´¥"}
                
        except Exception as e:
            return {"success": False, "error": f"Elasticsearchéƒ¨ç½²æ£€æŸ¥å¤±è´¥: {str(e)}"}
    
    def check_index_status(self) -> Dict[str, Any]:
        """æ£€æŸ¥ç´¢å¼•çŠ¶æ€å’Œæ–‡æ¡£æ•°é‡"""
        try:
            if not self.es:
                self.es = get_es()
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦å­˜åœ¨
            if not self.es.indices.exists(index=self.index_name):
                return {
                    "exists": False,
                    "document_count": 0,
                    "message": f"ç´¢å¼• '{self.index_name}' ä¸å­˜åœ¨"
                }
            
            # è·å–æ–‡æ¡£æ•°é‡
            try:
                count_result = self.es.count(index=self.index_name)
                document_count = count_result.get('count', 0)
                
                return {
                    "exists": True,
                    "document_count": document_count,
                    "message": f"ç´¢å¼• '{self.index_name}' åŒ…å« {document_count} ä¸ªæ–‡æ¡£"
                }
            except Exception as e:
                return {
                    "exists": True,
                    "document_count": 0,
                    "message": f"æ— æ³•è·å–æ–‡æ¡£æ•°é‡: {str(e)}"
                }
                
        except Exception as e:
            return {
                "exists": False,
                "document_count": 0,
                "message": f"æ£€æŸ¥ç´¢å¼•çŠ¶æ€å¤±è´¥: {str(e)}"
            }
    
    def step2_process_pdf(self, pdf_path: str) -> Dict[str, Any]:
        """æ­¥éª¤2: PDFå¤„ç†ï¼šæå–æ–‡æœ¬ã€å›¾ç‰‡å’Œè¡¨æ ¼"""
        print("ğŸ“„ æ­¥éª¤2: å¤„ç†PDFæ–‡ä»¶...")
        
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(pdf_path):
                return {"success": False, "error": f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}"}
            
            # æå–æ–‡æœ¬å†…å®¹
            print("  æå–æ–‡æœ¬å†…å®¹...")
            pdf_document = fitz.open(pdf_path)
            text_content = []
            
            for page_num in range(pdf_document.page_count):
                page = pdf_document.load_page(page_num)
                text = page.get_text()
                if text.strip():  # åªä¿å­˜éç©ºé¡µé¢
                    text_content.append({
                        "page_num": page_num + 1,
                        "content": text.strip(),
                        "content_type": "text"
                    })
            
            pdf_document.close()
            
            # æå–å›¾ç‰‡
            print("  æå–å›¾ç‰‡å†…å®¹...")
            try:
                images = extract_images_from_pdf(pdf_path)
                image_content = []
                for img in images:
                    image_content.append({
                        "page_num": img["page_num"] + 1,
                        "content": img.get("context_augmented_summary", img.get("summary", "")),
                        "content_type": "image",
                        "metadata": {
                            "image_index": img["image_index"],
                            "original_summary": img.get("summary", ""),
                            "page_context": img.get("page_context", "")
                        }
                    })
            except Exception as e:
                print(f"  âš ï¸ å›¾ç‰‡æå–å¤±è´¥: {e}")
                image_content = []
            
            # æå–è¡¨æ ¼
            print("  æå–è¡¨æ ¼å†…å®¹...")
            try:
                tables = extract_tables_from_pdf(pdf_path)
                table_content = []
                for table in tables:
                    table_content.append({
                        "page_num": table["page_num"] + 1,
                        "content": table.get("context_augmented_table", table.get("table_markdown", "")),
                        "content_type": "table",
                        "metadata": {
                            "table_index": table["table_index"],
                            "table_markdown": table.get("table_markdown", ""),
                            "page_context": table.get("page_context", "")
                        }
                    })
            except Exception as e:
                print(f"  âš ï¸ è¡¨æ ¼æå–å¤±è´¥: {e}")
                table_content = []
            
            all_content = text_content + image_content + table_content
            
            print(f"âœ… PDFå¤„ç†å®Œæˆ: æ–‡æœ¬é¡µé¢{len(text_content)}é¡µ, å›¾ç‰‡{len(image_content)}ä¸ª, è¡¨æ ¼{len(table_content)}ä¸ª")
            
            return {
                "success": True,
                "content": all_content,
                "stats": {
                    "text_pages": len(text_content),
                    "images": len(image_content),
                    "tables": len(table_content),
                    "total_items": len(all_content)
                }
            }
            
        except Exception as e:
            return {"success": False, "error": f"PDFå¤„ç†å¤±è´¥: {str(e)}"}
    
    def step3_chunk_content(self, content: List[Dict], chunk_size: int = 1024) -> Dict[str, Any]:
        """æ­¥éª¤3: å†…å®¹åˆ‡åˆ†ï¼šå°†å†…å®¹æ‹†åˆ†æˆå¯æ£€ç´¢çš„å•å…ƒ"""
        print("âœ‚ï¸ æ­¥éª¤3: åˆ‡åˆ†å†…å®¹...")
        
        try:
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=100,
                length_function=num_tokens_from_string
            )
            
            chunks = []
            chunk_id = 0
            
            for item in content:
                content_text = item["content"]
                content_type = item["content_type"]
                page_num = item["page_num"]
                
                if content_type == "text" and len(content_text) > chunk_size:
                    # å¯¹é•¿æ–‡æœ¬è¿›è¡Œåˆ‡åˆ†
                    text_chunks = text_splitter.split_text(content_text)
                    for i, chunk_text in enumerate(text_chunks):
                        chunks.append({
                            "id": f"chunk_{chunk_id}",
                            "content": chunk_text,
                            "content_type": content_type,
                            "page_num": page_num,
                            "chunk_index": i,
                            "metadata": item.get("metadata", {})
                        })
                        chunk_id += 1
                else:
                    # å›¾ç‰‡å’Œè¡¨æ ¼ä¸åˆ‡åˆ†ï¼Œç›´æ¥ä½œä¸ºä¸€ä¸ªå—
                    chunks.append({
                        "id": f"chunk_{chunk_id}",
                        "content": content_text,
                        "content_type": content_type,
                        "page_num": page_num,
                        "chunk_index": 0,
                        "metadata": item.get("metadata", {})
                    })
                    chunk_id += 1
            
            print(f"âœ… å†…å®¹åˆ‡åˆ†å®Œæˆ: å…±ç”Ÿæˆ{len(chunks)}ä¸ªå—")
            
            return {
                "success": True,
                "chunks": chunks,
                "total_chunks": len(chunks)
            }
            
        except Exception as e:
            return {"success": False, "error": f"å†…å®¹åˆ‡åˆ†å¤±è´¥: {str(e)}"}
    
    def step4_vectorize_content(self, chunks: List[Dict]) -> Dict[str, Any]:
        """æ­¥éª¤4: å‘é‡åŒ–ï¼šä¸ºæ–‡æœ¬ã€è¡¨æ ¼æ‘˜è¦å’Œå›¾ç‰‡æè¿°ç”Ÿæˆå‘é‡"""
        print("ğŸ”¢ æ­¥éª¤4: å‘é‡åŒ–å†…å®¹...")
        
        try:
            # å‡†å¤‡æ–‡æœ¬åˆ—è¡¨
            texts = [chunk["content"] for chunk in chunks]
            
            # æ‰¹é‡ç”Ÿæˆå‘é‡
            print(f"  æ­£åœ¨ä¸º{len(texts)}ä¸ªå—ç”Ÿæˆå‘é‡...")
            batch_size = 25
            all_vectors = []
            
            for i in range(0, len(texts), batch_size):
                batch_texts = texts[i:i + batch_size]
                batch_vectors = local_embedding(batch_texts)
                all_vectors.extend(batch_vectors)
                print(f"  å·²å¤„ç† {min(i + batch_size, len(texts))}/{len(texts)} ä¸ªå—")
            
            # å°†å‘é‡æ·»åŠ åˆ°å—ä¸­
            for chunk, vector in zip(chunks, all_vectors):
                chunk["vector"] = vector
            
            print(f"âœ… å‘é‡åŒ–å®Œæˆ: ç”Ÿæˆ{len(all_vectors)}ä¸ªå‘é‡")
            
            return {
                "success": True,
                "vectorized_chunks": chunks,
                "vector_count": len(all_vectors)
            }
            
        except Exception as e:
            return {"success": False, "error": f"å‘é‡åŒ–å¤±è´¥: {str(e)}"}
    
    def step5_index_to_elasticsearch(self, vectorized_chunks: List[Dict]) -> Dict[str, Any]:
        """æ­¥éª¤5: ç´¢å¼•ï¼šå°†å†…å®¹ä¸å‘é‡ä¸€èµ·å­˜å‚¨åˆ°Elasticsearch"""
        print("ğŸ“‡ æ­¥éª¤5: ç´¢å¼•åˆ°Elasticsearch...")
        
        try:
            indexed_count = 0
            batch_size = 25
            
            for i in range(0, len(vectorized_chunks), batch_size):
                batch = vectorized_chunks[i:i + batch_size]
                
                for chunk in batch:
                    body = {
                        "text": chunk["content"],
                        "vector": chunk["vector"],
                        "content_type": chunk["content_type"],
                        "page_num": chunk["page_num"],
                        "chunk_index": chunk["chunk_index"],
                        "metadata": chunk.get("metadata", {})
                    }
                    
                    retry = 0
                    while retry <= 5:
                        try:
                            self.es.index(index=self.index_name, id=chunk["id"], body=body)
                            indexed_count += 1
                            break
                        except Exception as e:
                            print(f"    é‡è¯•ç´¢å¼• {chunk['id']}: {e}")
                            retry += 1
                            time.sleep(1)
                
                print(f"  å·²ç´¢å¼• {min(i + batch_size, len(vectorized_chunks))}/{len(vectorized_chunks)} ä¸ªå—")
            
            print(f"âœ… ç´¢å¼•å®Œæˆ: æˆåŠŸç´¢å¼•{indexed_count}ä¸ªå—")
            
            return {
                "success": True,
                "indexed_count": indexed_count,
                "total_chunks": len(vectorized_chunks)
            }
            
        except Exception as e:
            return {"success": False, "error": f"ç´¢å¼•å¤±è´¥: {str(e)}"}
    
    def step6_hybrid_search(self, query: str, top_k: int = 10) -> Dict[str, Any]:
        """æ­¥éª¤6: æ£€ç´¢ï¼šæ”¯æŒæ··åˆæœç´¢ï¼ˆhybrid searchï¼‰"""
        print("ğŸ” æ­¥éª¤6: æ··åˆæœç´¢...")
        
        try:
            # æ‰§è¡Œæ··åˆæœç´¢
            search_results = elastic_search(query, self.index_name)
            
            # é™åˆ¶ç»“æœæ•°é‡
            search_results = search_results[:top_k]
            
            print(f"âœ… æ··åˆæœç´¢å®Œæˆ: æ‰¾åˆ°{len(search_results)}ä¸ªç›¸å…³ç»“æœ")
            
            return {
                "success": True,
                "search_results": search_results,
                "result_count": len(search_results)
            }
            
        except Exception as e:
            return {"success": False, "error": f"æ··åˆæœç´¢å¤±è´¥: {str(e)}"}
    
    def step7_rerank_results(self, query: str, search_results: List[Dict]) -> Dict[str, Any]:
        """æ­¥éª¤7: é‡æ’åºï¼šåº”ç”¨RRFæˆ–reranker modelåšæœ€ç»ˆæ’åº"""
        print("ğŸ¯ æ­¥éª¤7: é‡æ’åºç»“æœ...")
        
        try:
            # åº”ç”¨é‡æ’åº
            reranked_results = rerank(query, search_results)
            
            print(f"âœ… é‡æ’åºå®Œæˆ: é‡æ–°æ’åº{len(reranked_results)}ä¸ªç»“æœ")
            
            return {
                "success": True,
                "reranked_results": reranked_results,
                "result_count": len(reranked_results)
            }
            
        except Exception as e:
            # å¦‚æœé‡æ’åºå¤±è´¥ï¼Œè¿”å›åŸå§‹ç»“æœ
            print(f"âš ï¸ é‡æ’åºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç»“æœ: {e}")
            return {
                "success": True,
                "reranked_results": search_results,
                "result_count": len(search_results),
                "warning": "é‡æ’åºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æœç´¢ç»“æœ"
            }
    
    def step8_generate_answer(self, query: str, reranked_results: List[Dict]) -> Dict[str, Any]:
        """æ­¥éª¤8: å›ç­”ï¼šåŸºäºæ£€ç´¢ç»“æœç”Ÿæˆå¸¦å¼•ç”¨çš„å›ç­”"""
        print("ğŸ’¬ æ­¥éª¤8: ç”Ÿæˆç­”æ¡ˆ...")
        
        try:
            # å‡†å¤‡ä¸Šä¸‹æ–‡
            context_parts = []
            citations = []
            
            for i, result in enumerate(reranked_results[:5]):  # ä½¿ç”¨å‰5ä¸ªç»“æœ
                text = result.get("text", "")
                page_num = result.get("metadata", {}).get("page_num", "æœªçŸ¥")
                content_type = result.get("metadata", {}).get("content_type", "text")
                
                context_parts.append(f"[å¼•ç”¨{i+1}] {text}")
                citations.append({
                    "id": i + 1,
                    "page": page_num,
                    "type": content_type,
                    "content": text[:200] + "..." if len(text) > 200 else text
                })
            
            context = "\n\n".join(context_parts)
            
            # ç”Ÿæˆç­”æ¡ˆçš„æç¤ºè¯
            prompt = f"""åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜: {query}

æ£€ç´¢åˆ°çš„ç›¸å…³å†…å®¹:
{context}

è¯·ä½ ï¼š
1. åŸºäºæ£€ç´¢åˆ°çš„å†…å®¹å›ç­”é—®é¢˜
2. åœ¨ç­”æ¡ˆä¸­æ ‡æ³¨å¼•ç”¨æ¥æºï¼Œæ ¼å¼ä¸º[å¼•ç”¨X]
3. å¦‚æœæ£€ç´¢å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·è¯´æ˜
4. ä¿æŒç­”æ¡ˆå‡†ç¡®ã€ç®€æ´

ç­”æ¡ˆ:"""

            # è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ
            answer = ask_llm(prompt)
            
            print("âœ… ç­”æ¡ˆç”Ÿæˆå®Œæˆ")
            
            return {
                "success": True,
                "answer": answer,
                "citations": citations,
                "context_used": len(context_parts)
            }
            
        except Exception as e:
            return {"success": False, "error": f"ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {str(e)}"}
    
    def load_documents_only(self, pdf_paths: List[str], chunk_size: int = 1024) -> Dict[str, Any]:
        """ä»…åŠ è½½æ–‡æ¡£åˆ°Elasticsearchï¼Œä¸è¿›è¡ŒæŸ¥è¯¢
        
        Args:
            pdf_paths: PDFæ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œæ”¯æŒå•ä¸ªæ–‡ä»¶æˆ–å¤šä¸ªæ–‡ä»¶
            chunk_size: æ–‡æ¡£åˆ†å—å¤§å°
            
        Returns:
            åŒ…å«å¤„ç†ç»“æœçš„å­—å…¸
        """
        # ç¡®ä¿pdf_pathsæ˜¯åˆ—è¡¨
        if isinstance(pdf_paths, str):
            pdf_paths = [pdf_paths]
            
        print("=" * 80)
        print(f"ğŸ“š åŠ è½½æ–‡æ¡£åˆ°Elasticsearch (ä»…ç´¢å¼•æ¨¡å¼)")
        print(f"ğŸ“ å¾…å¤„ç†æ–‡ä»¶: {len(pdf_paths)}ä¸ª")
        for i, path in enumerate(pdf_paths, 1):
            print(f"  {i}. {path}")
        print("=" * 80)
        
        start_time = time.time()
        total_results = {
            "processed_files": [],
            "failed_files": [],
            "total_chunks": 0,
            "total_indexed": 0,
            "total_stats": {"text_pages": 0, "images": 0, "tables": 0}
        }
        
        # æ­¥éª¤1: éƒ¨ç½²Elasticsearch (åªéœ€è¦åšä¸€æ¬¡)
        step1_result = self.step1_deploy_elasticsearch()
        if not step1_result["success"]:
            return {"success": False, "error": f"Elasticsearchéƒ¨ç½²å¤±è´¥: {step1_result['error']}"}
        
        # å¤„ç†æ¯ä¸ªPDFæ–‡ä»¶
        for i, pdf_path in enumerate(pdf_paths, 1):
            print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶ {i}/{len(pdf_paths)}: {pdf_path}")
            print("-" * 60)
            
            try:
                # æ­¥éª¤2: å¤„ç†PDF
                step2_result = self.step2_process_pdf(pdf_path)
                if not step2_result["success"]:
                    error_msg = f"PDFå¤„ç†å¤±è´¥: {step2_result['error']}"
                    print(f"âŒ {error_msg}")
                    total_results["failed_files"].append({"file": pdf_path, "error": error_msg})
                    continue
                
                # æ­¥éª¤3: åˆ‡åˆ†å†…å®¹
                step3_result = self.step3_chunk_content(step2_result["content"], chunk_size)
                if not step3_result["success"]:
                    error_msg = f"å†…å®¹åˆ‡åˆ†å¤±è´¥: {step3_result['error']}"
                    print(f"âŒ {error_msg}")
                    total_results["failed_files"].append({"file": pdf_path, "error": error_msg})
                    continue
                
                # æ­¥éª¤4: å‘é‡åŒ–
                step4_result = self.step4_vectorize_content(step3_result["chunks"])
                if not step4_result["success"]:
                    error_msg = f"å‘é‡åŒ–å¤±è´¥: {step4_result['error']}"
                    print(f"âŒ {error_msg}")
                    total_results["failed_files"].append({"file": pdf_path, "error": error_msg})
                    continue
                
                # æ­¥éª¤5: ç´¢å¼•
                step5_result = self.step5_index_to_elasticsearch(step4_result["vectorized_chunks"])
                if not step5_result["success"]:
                    error_msg = f"ç´¢å¼•å¤±è´¥: {step5_result['error']}"
                    print(f"âŒ {error_msg}")
                    total_results["failed_files"].append({"file": pdf_path, "error": error_msg})
                    continue
                
                # æˆåŠŸå¤„ç†çš„æ–‡ä»¶
                file_result = {
                    "file": pdf_path,
                    "chunks": step3_result["total_chunks"],
                    "indexed": step5_result["indexed_count"],
                    "stats": step2_result["stats"]
                }
                total_results["processed_files"].append(file_result)
                total_results["total_chunks"] += step3_result["total_chunks"]
                total_results["total_indexed"] += step5_result["indexed_count"]
                
                # ç´¯åŠ ç»Ÿè®¡
                for key in total_results["total_stats"]:
                    total_results["total_stats"][key] += step2_result["stats"].get(key, 0)
                
                # ä¿å­˜å¤„ç†è¿‡çš„PDF
                self.processed_pdfs.append(pdf_path)
                
                print(f"âœ… å®Œæˆ: {step5_result['indexed_count']}ä¸ªå—å·²ç´¢å¼•")
                
            except Exception as e:
                error_msg = f"å¤„ç†å¼‚å¸¸: {str(e)}"
                print(f"âŒ {error_msg}")
                total_results["failed_files"].append({"file": pdf_path, "error": error_msg})
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        # æ·»åŠ æ‰§è¡Œæ—¶é—´å’ŒæˆåŠŸæ ‡è®°
        total_results["execution_time"] = execution_time
        total_results["success"] = True
        total_results["mode"] = "load_only"
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
        print("\n" + "=" * 80)
        print("ğŸ“Š æ‰¹é‡åŠ è½½å®Œæˆ!")
        print("=" * 80)
        print(f"â±ï¸  æ€»è€—æ—¶: {execution_time:.2f}ç§’")
        print(f"ğŸ“ å¤„ç†æ–‡ä»¶: {len(total_results['processed_files'])}/{len(pdf_paths)}ä¸ªæˆåŠŸ")
        print(f"ğŸ“‡ ç´¢å¼•ç»Ÿè®¡: {total_results['total_indexed']}ä¸ªæ–‡æ¡£å—å·²åŠ è½½åˆ° {self.index_name}")
        print(f"ğŸ“„ å†…å®¹ç»Ÿè®¡: æ–‡æœ¬é¡µé¢{total_results['total_stats']['text_pages']}é¡µ, " +
              f"å›¾ç‰‡{total_results['total_stats']['images']}ä¸ª, " + 
              f"è¡¨æ ¼{total_results['total_stats']['tables']}ä¸ª")
        
        if total_results["failed_files"]:
            print(f"\nâš ï¸  å¤±è´¥æ–‡ä»¶ ({len(total_results['failed_files'])}ä¸ª):")
            for failed in total_results["failed_files"]:
                print(f"  âŒ {failed['file']}: {failed['error']}")
        
        print("\nâœ… ç°åœ¨å¯ä»¥ä½¿ç”¨äº¤äº’æ¨¡å¼è¿›è¡ŒæŸ¥è¯¢!")
        print("   å‘½ä»¤: python pipeline.py --interactive")
        print("=" * 80)
        
        return total_results

    def run_complete_pipeline(self, pdf_path: str, query: str, 
                            chunk_size: int = 1024, top_k: int = 10) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„RAGæµæ°´çº¿"""
        print("=" * 80)
        print("ğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´çš„PDF RAGæµæ°´çº¿")
        print("=" * 80)
        
        start_time = time.time()
        results = {}
        
        # æ­¥éª¤1: éƒ¨ç½²Elasticsearch
        step1_result = self.step1_deploy_elasticsearch()
        if not step1_result["success"]:
            return {"success": False, "error": f"æ­¥éª¤1å¤±è´¥: {step1_result['error']}"}
        
        # æ­¥éª¤2: å¤„ç†PDF
        step2_result = self.step2_process_pdf(pdf_path)
        if not step2_result["success"]:
            return {"success": False, "error": f"æ­¥éª¤2å¤±è´¥: {step2_result['error']}"}
        results.update(step2_result)
        
        # æ­¥éª¤3: åˆ‡åˆ†å†…å®¹
        step3_result = self.step3_chunk_content(step2_result["content"], chunk_size)
        if not step3_result["success"]:
            return {"success": False, "error": f"æ­¥éª¤3å¤±è´¥: {step3_result['error']}"}
        results.update(step3_result)
        
        # æ­¥éª¤4: å‘é‡åŒ–
        step4_result = self.step4_vectorize_content(step3_result["chunks"])
        if not step4_result["success"]:
            return {"success": False, "error": f"æ­¥éª¤4å¤±è´¥: {step4_result['error']}"}
        results.update(step4_result)
        
        # æ­¥éª¤5: ç´¢å¼•
        step5_result = self.step5_index_to_elasticsearch(step4_result["vectorized_chunks"])
        if not step5_result["success"]:
            return {"success": False, "error": f"æ­¥éª¤5å¤±è´¥: {step5_result['error']}"}
        results.update(step5_result)
        
        # æ­¥éª¤6: æ··åˆæœç´¢
        step6_result = self.step6_hybrid_search(query, top_k)
        if not step6_result["success"]:
            return {"success": False, "error": f"æ­¥éª¤6å¤±è´¥: {step6_result['error']}"}
        results.update(step6_result)
        
        # æ­¥éª¤7: é‡æ’åº
        step7_result = self.step7_rerank_results(query, step6_result["search_results"])
        if not step7_result["success"]:
            return {"success": False, "error": f"æ­¥éª¤7å¤±è´¥: {step7_result['error']}"}
        results.update(step7_result)
        
        # æ­¥éª¤8: ç”Ÿæˆç­”æ¡ˆ
        step8_result = self.step8_generate_answer(query, step7_result["reranked_results"])
        if not step8_result["success"]:
            return {"success": False, "error": f"æ­¥éª¤8å¤±è´¥: {step8_result['error']}"}
        results.update(step8_result)
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        # æ·»åŠ æ‰§è¡Œæ—¶é—´
        results["execution_time"] = execution_time
        results["success"] = True
        
        # ä¿å­˜å¤„ç†è¿‡çš„PDF
        self.processed_pdfs.append(pdf_path)
        
        print("=" * 80)
        print(f"ğŸ‰ æµæ°´çº¿æ‰§è¡Œå®Œæˆ! æ€»è€—æ—¶: {execution_time:.2f}ç§’")
        print("=" * 80)
        
        return results
    
    def interactive_qa(self):
        """äº¤äº’å¼é—®ç­”æ¨¡å¼"""
        print("\n" + "=" * 60)
        print("ğŸ’¬ è¿›å…¥äº¤äº’å¼é—®ç­”æ¨¡å¼")
        print("è¾“å…¥é—®é¢˜å¼€å§‹å¯¹è¯ï¼Œè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("è¾“å…¥ 'history' æŸ¥çœ‹å¯¹è¯å†å²")
        print("=" * 60)
        
        while True:
            try:
                # è·å–ç”¨æˆ·è¾“å…¥
                user_input = input("\nğŸ¤” æ‚¨çš„é—®é¢˜: ").strip()
                
                if not user_input:
                    continue
                
                # é€€å‡ºå‘½ä»¤
                if user_input.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                    print("ğŸ‘‹ å†è§!")
                    break
                
                # æŸ¥çœ‹å†å²å‘½ä»¤
                if user_input.lower() in ['history', 'å†å²', 'h']:
                    self._show_chat_history()
                    continue
                
                print(f"\nğŸ” æ­£åœ¨å¤„ç†é—®é¢˜: {user_input}")
                
                # æŒ‡ä»£æ¶ˆè§£ï¼ˆå¦‚æœæœ‰å†å²å¯¹è¯ï¼‰
                query = user_input
                if self.chat_history:
                    try:
                        resolved_queries = coreference_resolution(user_input, self.chat_history)
                        if resolved_queries and len(resolved_queries) > 0:
                            query = resolved_queries[0]
                            if query != user_input:
                                print(f"ğŸ“ æŒ‡ä»£æ¶ˆè§£: {query}")
                    except Exception as e:
                        print(f"âš ï¸ æŒ‡ä»£æ¶ˆè§£å¤±è´¥: {e}")
                
                # æ‰§è¡Œæœç´¢å’Œå›ç­”
                start_time = time.time()
                
                # æ­¥éª¤6: æ··åˆæœç´¢
                step6_result = self.step6_hybrid_search(query, 10)
                if not step6_result["success"]:
                    print(f"âŒ æœç´¢å¤±è´¥: {step6_result['error']}")
                    continue
                
                # æ­¥éª¤7: é‡æ’åº
                step7_result = self.step7_rerank_results(query, step6_result["search_results"])
                
                # æ­¥éª¤8: ç”Ÿæˆç­”æ¡ˆ
                step8_result = self.step8_generate_answer(query, step7_result["reranked_results"])
                if not step8_result["success"]:
                    print(f"âŒ ç­”æ¡ˆç”Ÿæˆå¤±è´¥: {step8_result['error']}")
                    continue
                
                end_time = time.time()
                
                # æ˜¾ç¤ºç­”æ¡ˆ
                print("\n" + "=" * 50)
                print("ğŸ¤– AIå›ç­”:")
                print("=" * 50)
                print(step8_result["answer"])
                
                # æ˜¾ç¤ºå¼•ç”¨
                if step8_result.get("citations"):
                    print("\nğŸ“š å¼•ç”¨æ¥æº:")
                    for citation in step8_result["citations"]:
                        print(f"  [å¼•ç”¨{citation['id']}] ç¬¬{citation['page']}é¡µ ({citation['type']})")
                        print(f"    å†…å®¹: {citation['content']}")
                
                print(f"\nâ±ï¸ å“åº”æ—¶é—´: {end_time - start_time:.2f}ç§’")
                print("=" * 50)
                
                # ä¿å­˜åˆ°å†å²
                self.chat_history.append(f"user: {user_input}")
                self.chat_history.append(f"assistant: {step8_result['answer']}")
                
                # é™åˆ¶å†å²é•¿åº¦
                if len(self.chat_history) > 20:
                    self.chat_history = self.chat_history[-20:]
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ å†è§!")
                break
            except Exception as e:
                print(f"âŒ å¤„ç†é”™è¯¯: {e}")
                traceback.print_exc()
    
    def _show_chat_history(self):
        """æ˜¾ç¤ºèŠå¤©å†å²"""
        if not self.chat_history:
            print("ğŸ“ æš‚æ— å¯¹è¯å†å²")
            return
        
        print("\nğŸ“š å¯¹è¯å†å²:")
        print("-" * 50)
        for i, message in enumerate(self.chat_history[-10:]):  # æ˜¾ç¤ºæœ€è¿‘10æ¡
            if message.startswith("user:"):
                print(f"ğŸ¤” {message[5:].strip()}")
            elif message.startswith("assistant:"):
                answer = message[10:].strip()
                if len(answer) > 100:
                    answer = answer[:100] + "..."
                print(f"ğŸ¤– {answer}")
                print("-" * 30)


def main():
    """ä¸»å‡½æ•° - å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(description="PDF RAGæµæ°´çº¿")
    parser.add_argument("--pdf", type=str, help="PDFæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--pdf-dir", type=str, help="åŒ…å«PDFæ–‡ä»¶çš„ç›®å½•è·¯å¾„")
    parser.add_argument("--load-only", action="store_true", 
                       help="ä»…åŠ è½½æ–‡æ¡£åˆ°Elasticsearchï¼Œä¸è¿›è¡ŒæŸ¥è¯¢")
    parser.add_argument("--status", action="store_true",
                       help="æ£€æŸ¥ç´¢å¼•çŠ¶æ€å’Œæ–‡æ¡£æ•°é‡")
    parser.add_argument("--query", type=str, help="æŸ¥è¯¢é—®é¢˜")
    parser.add_argument("--index-name", type=str, default="rag_pipeline_index", 
                       help="Elasticsearchç´¢å¼•åç§°")
    parser.add_argument("--chunk-size", type=int, default=1024, 
                       help="æ–‡æ¡£åˆ†å—å¤§å°")
    parser.add_argument("--top-k", type=int, default=10, 
                       help="æ£€ç´¢ç»“æœæ•°é‡")
    parser.add_argument("--interactive", action="store_true", 
                       help="è¿›å…¥äº¤äº’å¼æ¨¡å¼")
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµæ°´çº¿å®ä¾‹
    pipeline = RAGPipeline(args.index_name)
    
    if args.interactive:
        # äº¤äº’å¼æ¨¡å¼
        print("ğŸ¤– PDF RAGæµæ°´çº¿ - äº¤äº’å¼æ¨¡å¼")
        
        # æ£€æŸ¥Elasticsearch
        step1_result = pipeline.step1_deploy_elasticsearch()
        if not step1_result["success"]:
            print(f"âŒ Elasticsearchè¿æ¥å¤±è´¥: {step1_result['error']}")
            return
        
        # æ£€æŸ¥ç´¢å¼•çŠ¶æ€
        index_status = pipeline.check_index_status()
        print(f"ğŸ“Š {index_status['message']}")
        
        if not index_status["exists"] or index_status["document_count"] == 0:
            print("\nâŒ æ— æ³•è¿›å…¥äº¤äº’æ¨¡å¼:")
            if not index_status["exists"]:
                print(f"   ç´¢å¼• '{args.index_name}' ä¸å­˜åœ¨")
            else:
                print(f"   ç´¢å¼• '{args.index_name}' ä¸­æ²¡æœ‰æ–‡æ¡£")
            
            print("\nğŸ’¡ è¯·å…ˆåŠ è½½ä¸€äº›æ–‡æ¡£:")
            print("   # åŠ è½½å•ä¸ªPDF")
            print(f"   python pipeline.py --pdf document.pdf --load-only --index-name {args.index_name}")
            print("")
            print("   # æ‰¹é‡åŠ è½½ç›®å½•ä¸­çš„PDF")
            print(f"   python pipeline.py --pdf-dir /path/to/pdfs --load-only --index-name {args.index_name}")
            print("")
            print("   # å®Œæ•´æµæ°´çº¿ (å¤„ç†+æŸ¥è¯¢)")
            print(f"   python pipeline.py --pdf document.pdf --query 'ä½ çš„é—®é¢˜' --index-name {args.index_name}")
            return
        
        print(f"âœ… ç´¢å¼•å‡†å¤‡å°±ç»ªï¼ŒåŒ…å« {index_status['document_count']} ä¸ªæ–‡æ¡£")
        pipeline.interactive_qa()
        
    elif args.status:
        # çŠ¶æ€æ£€æŸ¥æ¨¡å¼
        print("ğŸ“Š æ£€æŸ¥ç´¢å¼•çŠ¶æ€")
        print("=" * 50)
        
        # æ£€æŸ¥Elasticsearchè¿æ¥
        step1_result = pipeline.step1_deploy_elasticsearch()
        if not step1_result["success"]:
            print(f"âŒ Elasticsearchè¿æ¥å¤±è´¥: {step1_result['error']}")
            return
        
        # æ£€æŸ¥ç´¢å¼•çŠ¶æ€
        index_status = pipeline.check_index_status()
        print(f"ğŸ“‡ ç´¢å¼•åç§°: {args.index_name}")
        print(f"ğŸ“Š çŠ¶æ€: {index_status['message']}")
        
        if index_status["exists"]:
            if index_status["document_count"] > 0:
                print("âœ… çŠ¶æ€è‰¯å¥½ - å¯ä»¥è¿›å…¥äº¤äº’æ¨¡å¼")
                print(f"   å‘½ä»¤: python pipeline.py --interactive --index-name {args.index_name}")
            else:
                print("âš ï¸ ç´¢å¼•ä¸ºç©º - éœ€è¦åŠ è½½æ–‡æ¡£")
                print("ğŸ’¡ å»ºè®®æ“ä½œ:")
                print(f"   python pipeline.py --pdf document.pdf --load-only --index-name {args.index_name}")
        else:
            print("âŒ ç´¢å¼•ä¸å­˜åœ¨ - éœ€è¦å…ˆåˆ›å»ºå¹¶åŠ è½½æ–‡æ¡£")
            print("ğŸ’¡ å»ºè®®æ“ä½œ:")
            print(f"   python pipeline.py --pdf document.pdf --load-only --index-name {args.index_name}")
        
    elif args.load_only:
        # ä»…åŠ è½½æ¨¡å¼
        pdf_paths = []
        
        # æ”¶é›†PDFæ–‡ä»¶è·¯å¾„
        if args.pdf:
            pdf_paths.append(args.pdf)
        
        if args.pdf_dir:
            dir_pdfs = glob.glob(os.path.join(args.pdf_dir, "*.pdf"))
            pdf_paths.extend(dir_pdfs)
        
        if not pdf_paths:
            print("âŒ æœªæŒ‡å®šPDFæ–‡ä»¶ã€‚è¯·ä½¿ç”¨ --pdf æŒ‡å®šå•ä¸ªæ–‡ä»¶æˆ– --pdf-dir æŒ‡å®šç›®å½•")
            return
        
        # æ‰§è¡Œä»…åŠ è½½æ¨¡å¼
        result = pipeline.load_documents_only(
            pdf_paths=pdf_paths,
            chunk_size=args.chunk_size
        )
        
        if result["success"]:
            print(f"\nğŸ‰ æˆåŠŸåŠ è½½ {len(result['processed_files'])} ä¸ªæ–‡ä»¶åˆ°ç´¢å¼• {args.index_name}")
            
            if result["failed_files"]:
                print(f"âš ï¸  {len(result['failed_files'])} ä¸ªæ–‡ä»¶å¤„ç†å¤±è´¥")
        else:
            print(f"âŒ åŠ è½½å¤±è´¥: {result['error']}")
            
    elif args.pdf and args.query:
        # å®Œæ•´æµæ°´çº¿æ¨¡å¼
        result = pipeline.run_complete_pipeline(
            pdf_path=args.pdf,
            query=args.query,
            chunk_size=args.chunk_size,
            top_k=args.top_k
        )
        
        if result["success"]:
            print("\n" + "=" * 60)
            print("ğŸ¤– æœ€ç»ˆç­”æ¡ˆ:")
            print("=" * 60)
            print(result["answer"])
            
            if result.get("citations"):
                print("\nğŸ“š å¼•ç”¨æ¥æº:")
                for citation in result["citations"]:
                    print(f"  [å¼•ç”¨{citation['id']}] ç¬¬{citation['page']}é¡µ ({citation['type']})")
            
            print(f"\nğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
            print(f"  - å¤„ç†æ–‡æ¡£å—: {result.get('total_chunks', 0)}")
            print(f"  - æœç´¢ç»“æœ: {result.get('result_count', 0)}")
            print(f"  - æ‰§è¡Œæ—¶é—´: {result.get('execution_time', 0):.2f}ç§’")
            
        else:
            print(f"âŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {result['error']}")
    
    else:
        # æ˜¾ç¤ºå¸®åŠ©
        parser.print_help()
        print("\nä½¿ç”¨ç¤ºä¾‹:")
        print("  # æ£€æŸ¥ç´¢å¼•çŠ¶æ€")
        print("  python pipeline.py --status")
        print("")
        print("  # å®Œæ•´æµæ°´çº¿ (å¤„ç†+æŸ¥è¯¢)")
        print("  python pipeline.py --pdf document.pdf --query 'ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ'")
        print("")
        print("  # ä»…åŠ è½½å•ä¸ªæ–‡æ¡£")
        print("  python pipeline.py --pdf document.pdf --load-only")
        print("")
        print("  # æ‰¹é‡åŠ è½½ç›®å½•ä¸­çš„æ‰€æœ‰PDF")
        print("  python pipeline.py --pdf-dir /path/to/pdfs --load-only")
        print("")
        print("  # æ··åˆåŠ è½½ (å•ä¸ªæ–‡ä»¶+ç›®å½•)")
        print("  python pipeline.py --pdf doc1.pdf --pdf-dir /path/to/pdfs --load-only")
        print("")
        print("  # äº¤äº’å¼æŸ¥è¯¢æ¨¡å¼ (éœ€è¦å…ˆæœ‰æ–‡æ¡£)")
        print("  python pipeline.py --interactive")


if __name__ == "__main__":
    main()
