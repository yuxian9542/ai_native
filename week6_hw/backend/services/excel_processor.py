"""
Excelé¢„å¤„ç†æœåŠ¡
æ™ºèƒ½è¯†åˆ«æ— å…³è¡Œã€å¤„ç†å¤šçº§è¡¨å¤´ã€æ‹†åˆ†åˆå¹¶å•å…ƒæ ¼ã€è¾“å‡ºæ ‡å‡†äºŒç»´è¡¨
"""
import pandas as pd
import openpyxl
from pathlib import Path
from typing import Tuple, List, Dict, Any
from backend.utils.openai_client import openai_client
from backend.config import settings
from backend.utils.logger import logger
import json


class ExcelProcessor:
    """Excelæ–‡ä»¶é¢„å¤„ç†å™¨"""
    
    async def process_excel(self, file_path: str, output_path: str) -> Dict[str, Any]:
        """
        å¤„ç†Excelæ–‡ä»¶ï¼ˆæ”¯æŒå¤šå·¥ä½œè¡¨å’ŒSchemaåˆ†å‰²ï¼‰
        
        Args:
            file_path: è¾“å…¥Excelæ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºExcelæ–‡ä»¶è·¯å¾„
            
        Returns:
            å¤„ç†æ—¥å¿—ä¿¡æ¯
        """
        print(f"ğŸš€ å¼€å§‹å¤„ç†Excelæ–‡ä»¶: {file_path}")
        print(f"å¼€å§‹å¤„ç†Excelæ–‡ä»¶: {file_path}")
        
        # åŠ è½½å·¥ä½œç°¿
        wb = openpyxl.load_workbook(file_path)
        sheet_names = wb.sheetnames
        print(f"ğŸ“Š å‘ç° {len(sheet_names)} ä¸ªå·¥ä½œè¡¨: {sheet_names}")
        print(f"excel_sheets_detected å‘ç° {len(sheet_names)} ä¸ªå·¥ä½œè¡¨: {sheet_names}")
        
        # åˆ›å»ºè¾“å‡ºå·¥ä½œç°¿
        output_wb = openpyxl.Workbook()
        output_wb.remove(output_wb.active)  # åˆ é™¤é»˜è®¤å·¥ä½œè¡¨
        
        processed_sheets = {}
        total_log = {
            "total_sheets": len(sheet_names),
            "processed_sheets": 0,
            "schema_split_sheets": 0,
            "skipped_rows": 0,
            "header_row": 0,
            "merged_cells_count": 0,
            "final_rows": 0,
            "final_columns": 0,
            "sheet_details": {}
        }
        
        for sheet_name in sheet_names:
            try:
                print(f"ğŸ”„ å¤„ç†å·¥ä½œè¡¨: {sheet_name}")
                print(f"sheet_processing_start å¼€å§‹å¤„ç†å·¥ä½œè¡¨: {sheet_name}")
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦Schemaåˆ†å‰²
                print(f"ğŸ” æ£€æŸ¥Schemaåˆ†å‰²éœ€æ±‚: {sheet_name}")
                needs_schema_split = await self._check_schema_split_needed(file_path, sheet_name)
                print(f"ğŸ“Š Schemaåˆ†å‰²æ£€æµ‹ç»“æœ: {'éœ€è¦åˆ†å‰²' if needs_schema_split else 'ä¸éœ€è¦åˆ†å‰²'}")
                print(f"schema_split_check å·¥ä½œè¡¨ {sheet_name} Schemaåˆ†å‰²æ£€æµ‹: {'éœ€è¦åˆ†å‰²' if needs_schema_split else 'ä¸éœ€è¦åˆ†å‰²'}")
                
                if needs_schema_split:
                    print(f"âœ‚ï¸ æ£€æµ‹åˆ°Schemaä¸ä¸€è‡´ï¼Œå°†åˆ é™¤è¡¨æœ«å°¾éƒ¨åˆ†: {sheet_name}")
                    print(f"schema_trim_start å¼€å§‹åˆ é™¤è¡¨æœ«å°¾éƒ¨åˆ†: {sheet_name}")
                    
                    # å…ˆå¤„ç†åˆå¹¶å•å…ƒæ ¼
                    print(f"ğŸ”§ å¤„ç†åˆå¹¶å•å…ƒæ ¼: {sheet_name}")
                    wb = openpyxl.load_workbook(file_path)
                    ws = wb[sheet_name]
                    merged_cells_count = len(ws.merged_cells.ranges)
                    print(f"ğŸ“Š å‘ç° {merged_cells_count} ä¸ªåˆå¹¶å•å…ƒæ ¼")
                    print(f"merged_cells_processing å·¥ä½œè¡¨ {sheet_name} å‘ç° {merged_cells_count} ä¸ªåˆå¹¶å•å…ƒæ ¼")
                    
                    self._unmerge_cells_with_fill(ws)
                    print(f"âœ… åˆå¹¶å•å…ƒæ ¼å¤„ç†å®Œæˆ")
                    print(f"merged_cells_processed å·¥ä½œè¡¨ {sheet_name} åˆå¹¶å•å…ƒæ ¼å¤„ç†å®Œæˆ")
                    
                    # ä¿å­˜å¤„ç†åçš„ä¸´æ—¶æ–‡ä»¶
                    temp_file = file_path.replace('.xlsx', f'_temp_{sheet_name}.xlsx')
                    wb.save(temp_file)
                    print(f"ğŸ’¾ ä¿å­˜ä¸´æ—¶æ–‡ä»¶: {temp_file}")
                    
                    try:
                        # è¯»å–å®Œæ•´æ•°æ®
                        df_full = pd.read_excel(temp_file, sheet_name=sheet_name, header=None)
                        print(f"ğŸ“Š è¯»å–å®Œæ•´æ•°æ®: {len(df_full)}è¡Œ Ã— {len(df_full.columns)}åˆ—")
                        print(f"full_data_loaded è¯»å–å®Œæ•´æ•°æ®: {len(df_full)}è¡Œ Ã— {len(df_full.columns)}åˆ—")
                        
                        # è·å–ä¸»è¦æ•°æ®éƒ¨åˆ†ï¼ˆåˆ é™¤æœ«å°¾éƒ¨åˆ†ï¼‰
                        main_data = await self._get_main_data_section(df_full, sheet_name)
                        print(f"ğŸ“Š ä¸»è¦æ•°æ®éƒ¨åˆ†: {len(main_data)}è¡Œ Ã— {len(main_data.columns)}åˆ—")
                        print(f"main_data_extracted ä¸»è¦æ•°æ®éƒ¨åˆ†: {len(main_data)}è¡Œ Ã— {len(main_data.columns)}åˆ—")
                        
                        # å¤„ç†ä¸»è¦æ•°æ®
                        processed_df = await self._process_single_sheet_data_from_df(main_data, sheet_name)
                        processed_df = self._clean_dataframe(processed_df)
                        print(f"ğŸ“Š æ•°æ®å¤„ç†å®Œæˆ: {len(processed_df)}è¡Œ Ã— {len(processed_df.columns)}åˆ—")
                        print(f"data_processing_complete æ•°æ®å¤„ç†å®Œæˆ: {len(processed_df)}è¡Œ Ã— {len(processed_df.columns)}åˆ—")
                        
                        # æ·»åŠ åˆ°è¾“å‡ºå·¥ä½œç°¿ï¼ˆä½¿ç”¨åŸå·¥ä½œè¡¨åç§°ï¼‰
                        ws = output_wb.create_sheet(title=sheet_name)
                        self._write_dataframe_to_worksheet(ws, processed_df)
                        print(f"ğŸ“ å†™å…¥è¾“å‡ºå·¥ä½œç°¿: {sheet_name}")
                        
                        # è®°å½•ä¿¡æ¯
                        processed_sheets[sheet_name] = {
                            "rows": len(processed_df),
                            "columns": len(processed_df.columns),
                            "status": "success",
                            "type": "trimmed"
                        }
                        
                        total_log["processed_sheets"] += 1
                        total_log["final_rows"] += len(processed_df)
                        total_log["final_columns"] = max(total_log["final_columns"], len(processed_df.columns))
                        
                        print(f"âœ… è¡¨æœ«å°¾éƒ¨åˆ†åˆ é™¤å®Œæˆ: {sheet_name}")
                        print(f"schema_trim_complete è¡¨æœ«å°¾éƒ¨åˆ†åˆ é™¤å®Œæˆ: {sheet_name}")
                    
                    finally:
                        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                        Path(temp_file).unlink(missing_ok=True)
                        print(f"ğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_file}")
                        print(f"temp_file_cleaned æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_file}")
                        
                else:
                    # ä½¿ç”¨åŸæœ‰é€»è¾‘å¤„ç†å•ä¸ªå·¥ä½œè¡¨
                    print(f"ğŸ“‹ ä½¿ç”¨æ ‡å‡†å¤„ç†é€»è¾‘: {sheet_name}")
                    print(f"normal_processing_start å¼€å§‹æ ‡å‡†å¤„ç†: {sheet_name}")
                    
                    sheet_log = await self._process_single_sheet(file_path, None, sheet_name)
                    print(f"âœ… å•å·¥ä½œè¡¨å¤„ç†å®Œæˆ: {sheet_name}")
                    print(f"normal_processing_complete å•å·¥ä½œè¡¨å¤„ç†å®Œæˆ: {sheet_name}")
                    
                    # è¯»å–å¤„ç†åçš„æ•°æ®ï¼Œå…ˆå¤„ç†åˆå¹¶å•å…ƒæ ¼å’Œå¤šçº§è¡¨å¤´
                    df = await self._process_single_sheet_data(file_path, sheet_name)
                    df = self._clean_dataframe(df)
                    print(f"ğŸ“Š æ•°æ®æ¸…ç†å®Œæˆ: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
                    print(f"data_cleaning_complete æ•°æ®æ¸…ç†å®Œæˆ: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
                    
                    # æ·»åŠ åˆ°è¾“å‡ºå·¥ä½œç°¿
                    ws = output_wb.create_sheet(title=sheet_name)
                    self._write_dataframe_to_worksheet(ws, df)
                    print(f"ğŸ“ å†™å…¥è¾“å‡ºå·¥ä½œç°¿: {sheet_name}")
                    
                    # è®°å½•å·¥ä½œè¡¨ä¿¡æ¯
                    processed_sheets[sheet_name] = {
                        "rows": len(df),
                        "columns": len(df.columns),
                        "status": "success",
                        "type": "normal"
                    }
                    
                    total_log["processed_sheets"] += 1
                    total_log["final_rows"] += len(df)
                    total_log["final_columns"] = max(total_log["final_columns"], len(df.columns))
                    total_log["sheet_details"][sheet_name] = sheet_log
                    
                    print(f"ğŸ“Š ç´¯è®¡ç»Ÿè®¡: {total_log['processed_sheets']}ä¸ªå·¥ä½œè¡¨, {total_log['final_rows']}è¡Œ, {total_log['final_columns']}åˆ—")
                
            except Exception as e:
                print(f"âŒ å¤„ç†å·¥ä½œè¡¨ {sheet_name} å¤±è´¥: {e}")
                logger.log_error("sheet_processing_failed", f"å·¥ä½œè¡¨ {sheet_name} å¤„ç†å¤±è´¥: {e}")
                processed_sheets[sheet_name] = {
                    "rows": 0,
                    "columns": 0,
                    "status": "error",
                    "error": str(e)
                }
        
        # ä¿å­˜å¤„ç†åçš„Excelæ–‡ä»¶
        print(f"ğŸ’¾ ä¿å­˜è¾“å‡ºæ–‡ä»¶: {output_path}")
        print(f"output_file_saving ä¿å­˜è¾“å‡ºæ–‡ä»¶: {output_path}")
        
        Path(output_path).parent.mkdir(parents=True, exist_ok=True)
        output_wb.save(output_path)
        
        print(f"âœ… è¾“å‡ºæ–‡ä»¶ä¿å­˜å®Œæˆ: {output_path}")
        print(f"output_file_saved è¾“å‡ºæ–‡ä»¶ä¿å­˜å®Œæˆ: {output_path}")
        
        total_log["processed_sheets_info"] = processed_sheets
        
        # æ‰“å°æœ€ç»ˆç»Ÿè®¡
        print(f"\nğŸ“Š å¤„ç†å®Œæˆç»Ÿè®¡:")
        print(f"  - æ€»å·¥ä½œè¡¨æ•°: {total_log['total_sheets']}")
        print(f"  - æˆåŠŸå¤„ç†: {total_log['processed_sheets']}")
        print(f"  - Schemaåˆ†å‰²å·¥ä½œè¡¨: {total_log['schema_split_sheets']}")
        print(f"  - æ€»è¡Œæ•°: {total_log['final_rows']}")
        print(f"  - æœ€å¤§åˆ—æ•°: {total_log['final_columns']}")
        
        print(f"excel_processing_complete Excelå¤„ç†å®Œæˆ: {total_log['processed_sheets']}ä¸ªå·¥ä½œè¡¨, {total_log['final_rows']}è¡Œ, {total_log['final_columns']}åˆ—")
        
        return {
            "total_sheets": total_log['total_sheets'],
            "successful_sheets": total_log['processed_sheets'],
            "processed_sheets": total_log['processed_sheets'],
            "schema_split_sheets": total_log['schema_split_sheets'],
            "total_rows": total_log['final_rows'],
            "max_columns": total_log['final_columns'],
            "sheet_details": total_log['sheet_details']
        }
    
    async def _process_single_sheet(self, file_path: str, output_path: str, sheet_name: str) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªå·¥ä½œè¡¨ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        
        Args:
            file_path: è¾“å…¥Excelæ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºExcelæ–‡ä»¶è·¯å¾„
            sheet_name: å·¥ä½œè¡¨åç§°
            
        Returns:
            å¤„ç†æ—¥å¿—ä¿¡æ¯
        """
        log = {
            "skipped_rows": 0,
            "header_row": 0,
            "merged_cells_count": 0,
            "final_rows": 0,
            "final_columns": 0
        }
        
        # 1. è¯†åˆ«æ— å…³è¡Œ
        skip_rows, header_row = await self._identify_irrelevant_rows(file_path, sheet_name)
        log["skipped_rows"] = len(skip_rows)
        log["header_row"] = header_row
        
        # 2. å¤„ç†åˆå¹¶å•å…ƒæ ¼å’Œå¤šçº§è¡¨å¤´
        wb = openpyxl.load_workbook(file_path)
        ws = wb[sheet_name]
        
        # ç»Ÿè®¡åˆå¹¶å•å…ƒæ ¼
        log["merged_cells_count"] = len(ws.merged_cells.ranges)
        
        # æ‹†åˆ†åˆå¹¶å•å…ƒæ ¼
        self._unmerge_cells(ws)
        
        # 3. è¯»å–æ•°æ®
        df = pd.read_excel(
            file_path,
            sheet_name=sheet_name,
            skiprows=skip_rows,
            header=None if header_row in skip_rows else 0
        )
        
        # 4. å¤„ç†åˆ—å
        if header_row in skip_rows:
            # å¦‚æœè¡¨å¤´è¢«è·³è¿‡äº†ï¼Œä½¿ç”¨é»˜è®¤åˆ—å
            df.columns = [f"Column_{i}" for i in range(len(df.columns))]
        else:
            # å¦‚æœè¡¨å¤´æ²¡æœ‰è¢«è·³è¿‡ï¼Œpandasä¼šè‡ªåŠ¨ä½¿ç”¨ç¬¬ä¸€è¡Œä½œä¸ºåˆ—å
            # æ¸…ç†åˆ—åï¼Œç¡®ä¿å®ƒä»¬æ˜¯å­—ç¬¦ä¸²ä¸”å”¯ä¸€
            df.columns = [str(col).strip() if col is not None else f"Column_{i}" 
                         for i, col in enumerate(df.columns)]
        
        # 5. æ¸…ç†æ•°æ®
        df = self._clean_dataframe(df)
        
        log["final_rows"] = len(df)
        log["final_columns"] = len(df.columns)
        
        # 6. ä¿å­˜ä¸ºExcelï¼ˆå¦‚æœæŒ‡å®šäº†è¾“å‡ºè·¯å¾„ï¼‰
        if output_path:
            Path(output_path).parent.mkdir(parents=True, exist_ok=True)
            df.to_excel(output_path, index=False, engine='openpyxl')
        
        return log
    
    async def _process_single_sheet_data(self, file_path: str, sheet_name: str) -> pd.DataFrame:
        """
        å¤„ç†å•ä¸ªå·¥ä½œè¡¨çš„æ•°æ®ï¼ŒåŒ…æ‹¬åˆå¹¶å•å…ƒæ ¼å’Œå¤šçº§è¡¨å¤´
        
        Args:
            file_path: Excelæ–‡ä»¶è·¯å¾„
            sheet_name: å·¥ä½œè¡¨åç§°
            
        Returns:
            å¤„ç†åçš„DataFrame
        """
        print(f"ğŸ”§ å¤„ç†å•å·¥ä½œè¡¨æ•°æ®: {sheet_name}")
        
        # 1. å¤„ç†åˆå¹¶å•å…ƒæ ¼
        wb = openpyxl.load_workbook(file_path)
        ws = wb[sheet_name]
        
        # ç»Ÿè®¡åˆå¹¶å•å…ƒæ ¼
        merged_cells_count = len(ws.merged_cells.ranges)
        print(f"ğŸ“Š å‘ç° {merged_cells_count} ä¸ªåˆå¹¶å•å…ƒæ ¼")
        
        # æ‹†åˆ†åˆå¹¶å•å…ƒæ ¼
        self._unmerge_cells_with_fill(ws)
        print(f"âœ… åˆå¹¶å•å…ƒæ ¼å¤„ç†å®Œæˆ")
        
        # å°†å…¬å¼è½¬æ¢ä¸ºå€¼
        self._convert_formulas_to_values(ws)
        print(f"âœ… å…¬å¼è½¬å€¼å¤„ç†å®Œæˆ")
        
        # 2. ä¿å­˜å¤„ç†åçš„ä¸´æ—¶æ–‡ä»¶
        temp_file = file_path.replace('.xlsx', f'_temp_{sheet_name}.xlsx')
        wb.save(temp_file)
        print(f"ğŸ’¾ ä¿å­˜ä¸´æ—¶æ–‡ä»¶: {temp_file}")
        
        try:
            # 3. è¯»å–æ•°æ®å¹¶å¤„ç†å¤šçº§è¡¨å¤´
            df = pd.read_excel(temp_file, sheet_name=sheet_name, header=None)
            print(f"ğŸ“Š è¯»å–æ•°æ®: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
            
            # 4. æ£€æµ‹æ ‡ç­¾è¡Œå’Œè¡¨å¤´
            if len(df) >= 2:
                # é¦–å…ˆæ£€æµ‹æ ‡ç­¾è¡Œ
                label_rows = self._detect_label_rows(df)
                print(f"ğŸ” æ£€æµ‹åˆ°æ ‡ç­¾è¡Œ: {label_rows}")
                
                if label_rows:
                    # ç§»é™¤æ ‡ç­¾è¡Œ
                    df = df.drop(label_rows).reset_index(drop=True)
                    print(f"ğŸ—‘ï¸ å·²ç§»é™¤æ ‡ç­¾è¡Œ: {label_rows}")
                
                # æ£€æŸ¥å‰©ä½™æ•°æ®æ˜¯å¦è¶³å¤Ÿ
                if len(df) >= 2:
                    # æ£€æŸ¥å‰ä¸¤è¡Œæ˜¯å¦æ„æˆå¤šçº§è¡¨å¤´
                    first_row = df.iloc[0].tolist()
                    second_row = df.iloc[1].tolist()
                    
                    print(f"ğŸ” æ£€æŸ¥å¤šçº§è¡¨å¤´:")
                    print(f"  ç¬¬ä¸€è¡Œ: {first_row}")
                    print(f"  ç¬¬äºŒè¡Œ: {second_row}")
                    
                    # åˆ¤æ–­æ˜¯å¦æ˜¯å¤šçº§è¡¨å¤´
                    is_multi_level = await self._detect_multi_level_headers(df, 0)
                    
                    if len(is_multi_level) > 1:
                        print(f"ğŸ“‹ æ£€æµ‹åˆ°å¤šçº§è¡¨å¤´ï¼Œè¿›è¡Œåˆå¹¶")
                        # åˆå¹¶å¤šçº§è¡¨å¤´
                        merged_headers = self._merge_multi_level_headers(df, is_multi_level)
                        df.columns = merged_headers
                        # è·³è¿‡æ‰€æœ‰è¡¨å¤´è¡Œ
                        df = df.iloc[max(is_multi_level)+1:].reset_index(drop=True)
                        print(f"ğŸ“‹ åˆå¹¶åçš„è¡¨å¤´: {merged_headers}")
                    else:
                        print(f"ğŸ“‹ ä½¿ç”¨å•çº§è¡¨å¤´")
                        # ä½¿ç”¨ç¬¬ä¸€è¡Œä½œä¸ºåˆ—å
                        df.columns = [str(col).strip() if pd.notna(col) and str(col).strip() else f"Column_{i}" 
                                     for i, col in enumerate(df.iloc[0])]
                        df = df.iloc[1:].reset_index(drop=True)
                        print(f"ğŸ“‹ åˆ—å: {df.columns.tolist()}")
                else:
                    print(f"âš ï¸ ç§»é™¤æ ‡ç­¾è¡Œåæ•°æ®ä¸è¶³ï¼Œä½¿ç”¨é»˜è®¤åˆ—å")
                    df.columns = [f"Column_{i}" for i in range(len(df.columns))]
            else:
                print(f"âš ï¸ æ•°æ®è¡Œæ•°ä¸è¶³ï¼Œä½¿ç”¨é»˜è®¤åˆ—å")
                df.columns = [f"Column_{i}" for i in range(len(df.columns))]
            
            print(f"âœ… å•å·¥ä½œè¡¨æ•°æ®å¤„ç†å®Œæˆ: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
            return df
            
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            Path(temp_file).unlink(missing_ok=True)
            print(f"ğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶: {temp_file}")
    
    def _convert_formulas_to_values(self, ws):
        """
        å°†å·¥ä½œè¡¨ä¸­çš„å…¬å¼è½¬æ¢ä¸ºå€¼
        
        Args:
            ws: openpyxlå·¥ä½œè¡¨å¯¹è±¡
        """
        print(f"ğŸ”„ å¼€å§‹è½¬æ¢å…¬å¼ä¸ºå€¼")
        
        formula_count = 0
        converted_count = 0
        
        # éå†æ‰€æœ‰å•å…ƒæ ¼
        for row in ws.iter_rows():
            for cell in row:
                if cell.data_type == 'f':  # å…¬å¼ç±»å‹
                    formula_count += 1
                    try:
                        # è·å–å…¬å¼çš„è®¡ç®—å€¼
                        if cell.value is not None:
                            # å°†å…¬å¼æ›¿æ¢ä¸ºè®¡ç®—å€¼
                            cell.value = cell.value
                            converted_count += 1
                    except Exception as e:
                        print(f"âš ï¸ å…¬å¼è½¬æ¢å¤±è´¥: {cell.coordinate} = {cell.value}, é”™è¯¯: {e}")
                        # å¦‚æœå…¬å¼è®¡ç®—å¤±è´¥ï¼Œå°è¯•è·å–æ˜¾ç¤ºå€¼
                        try:
                            if hasattr(cell, 'value') and cell.value is not None:
                                # ä¿æŒåŸå€¼ï¼Œä½†æ ‡è®°ä¸ºå·²å¤„ç†
                                converted_count += 1
                        except:
                            pass
        
        print(f"ğŸ“Š å…¬å¼è½¬æ¢ç»Ÿè®¡: å‘ç° {formula_count} ä¸ªå…¬å¼, æˆåŠŸè½¬æ¢ {converted_count} ä¸ª")
    
    def _detect_label_rows(self, df: pd.DataFrame) -> List[int]:
        """
        æ£€æµ‹æ ‡ç­¾è¡Œï¼ˆéœ€è¦ç§»é™¤çš„è¡Œï¼‰
        
        Args:
            df: DataFrame
            
        Returns:
            éœ€è¦ç§»é™¤çš„è¡Œç´¢å¼•åˆ—è¡¨
        """
        label_rows = []
        
        # æ£€æŸ¥å‰å‡ è¡Œæ˜¯å¦æ˜¯æ ‡ç­¾è¡Œ
        for i in range(min(5, len(df))):  # æœ€å¤šæ£€æŸ¥å‰5è¡Œ
            row = df.iloc[i]
            
            # å°†è¡Œè½¬æ¢ä¸ºæ–‡æœ¬
            row_text = ' '.join([str(x) for x in row if pd.notna(x)])
            non_null_count = row.notna().sum()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯æ ‡ç­¾è¡Œçš„ç‰¹å¾
            is_label = False
            
            # ç‰¹å¾1: å‰ä¸¤åˆ—ä¸ºç©ºï¼ˆé‡è¦ç‰¹å¾ï¼‰
            if len(row) >= 2 and pd.isna(row.iloc[0]) and pd.isna(row.iloc[1]):
                is_label = True
                print(f"  ğŸ“‹ è¡Œ{i} è¯†åˆ«ä¸ºæ ‡ç­¾è¡Œ: å‰ä¸¤åˆ—ä¸ºç©º")
            
            # ç‰¹å¾1.5: å‰ä¸¤åˆ—ä¸ºç©ºä¸”åŒ…å«ç‰¹å®šå…³é”®è¯
            elif len(row) >= 2 and pd.isna(row.iloc[0]) and pd.isna(row.iloc[1]) and \
                 any(keyword in row_text for keyword in ['ç¼–åˆ¶', 'å®¡æ ¸', 'æ—¥æœŸ']):
                is_label = True
                print(f"  ğŸ“‹ è¡Œ{i} è¯†åˆ«ä¸ºæ ‡ç­¾è¡Œ: å‰ä¸¤åˆ—ä¸ºç©ºä¸”åŒ…å«å…³é”®è¯")
            
            # ç‰¹å¾2: å¤§éƒ¨åˆ†åˆ—éƒ½æ˜¯NaNï¼ˆè¿™æ˜¯æœ€é‡è¦çš„ç‰¹å¾ï¼‰
            elif non_null_count <= 2:  # åªæœ‰2ä¸ªæˆ–æ›´å°‘çš„éç©ºå€¼
                is_label = True
            
            # ç‰¹å¾3: åŒ…å«"è¡¨"ã€"å›¾"ã€"å•ä½"ç­‰å…³é”®è¯ä¸”å¤§éƒ¨åˆ†åˆ—ä¸ºç©ºæˆ–åˆå¹¶å•å…ƒæ ¼åˆ—æ•°è¶…è¿‡5
            elif any(keyword in row_text for keyword in ['è¡¨', 'å›¾', 'å•ä½ï¼š', 'å•ä½:', 'æ•°æ®æ¥æº', 'ç»Ÿè®¡', 'æŠ¥å‘Š', 'ç¼–åˆ¶', 'å®¡æ ¸']) and (non_null_count <= 3 or self._count_merged_cells_in_row(df, i) > 5):
                is_label = True
            
            # ç‰¹å¾4: æ–‡æœ¬é•¿åº¦å¾ˆé•¿ä¸”å¤§éƒ¨åˆ†åˆ—ä¸ºç©ºæˆ–åˆå¹¶å•å…ƒæ ¼åˆ—æ•°è¶…è¿‡5ï¼Œå¯èƒ½æ˜¯æ ‡é¢˜
            elif len(row_text) > 50 and (non_null_count <= 3 or self._count_merged_cells_in_row(df, i) > 5):
                is_label = True
            
            # ç‰¹å¾5: åŒ…å«"å¢é•¿æƒ…å†µ"ã€"ç»Ÿè®¡è¡¨"ç­‰æè¿°æ€§æ–‡æœ¬ä¸”å¤§éƒ¨åˆ†åˆ—ä¸ºç©ºæˆ–åˆå¹¶å•å…ƒæ ¼åˆ—æ•°è¶…è¿‡5
            elif any(keyword in row_text for keyword in ['å¢é•¿æƒ…å†µ', 'ç»Ÿè®¡è¡¨', 'æ±‡æ€»è¡¨', 'æ˜ç»†è¡¨', 'åˆ†æè¡¨', 'æ±‡æ€»', 'æ„è§', 'æ—¥å¿—', 'å‘ç”µ']) and (non_null_count <= 3 or self._count_merged_cells_in_row(df, i) > 5):
                is_label = True
            
            # ç‰¹å¾6: åŒ…å«"åºå·"ã€"å§“å"ç­‰è¡¨å¤´å…³é”®è¯ä½†å¤§éƒ¨åˆ†åˆ—ä¸ºç©ºæˆ–åˆå¹¶å•å…ƒæ ¼åˆ—æ•°è¶…è¿‡5ï¼ˆå¯èƒ½æ˜¯æ ‡é¢˜è¡Œï¼‰
            elif any(keyword in row_text for keyword in ['åºå·', 'å§“å', 'å­¦å·', 'ç­çº§']) and (non_null_count <= 3 or self._count_merged_cells_in_row(df, i) > 5):
                is_label = True
            
            # æ’é™¤çœŸæ­£çš„è¡¨å¤´è¡Œï¼šå¦‚æœåŒ…å«å¸¸è§è¡¨å¤´å…³é”®è¯ä¸”å¤§éƒ¨åˆ†åˆ—éƒ½æœ‰å€¼ï¼Œåˆ™ä¸æ˜¯æ ‡ç­¾è¡Œ
            if any(keyword in row_text for keyword in ['åºå·', 'å§“å', 'å­¦å·', 'ç­çº§', 'å¯¼å¸ˆ', 'ç»„åˆ«', 'é¢˜ç›®', 'æ„è§', 'å¤‡æ³¨', 'ç”µæœº', 'ç¼–å·', 'å‘ç”µé‡', 'å‘ç”µå°æ—¶']) and non_null_count >= 6:
                is_label = False
            
            if is_label:
                label_rows.append(i)
                merged_count = self._count_merged_cells_in_row(df, i)
                print(f"  ğŸ“‹ è¡Œ{i} è¯†åˆ«ä¸ºæ ‡ç­¾è¡Œ: {row_text[:50]}... (éç©ºåˆ—: {non_null_count}/{len(row)}, åˆå¹¶å•å…ƒæ ¼: {merged_count})")
        
        return label_rows
    
    def _count_merged_cells_in_row(self, df: pd.DataFrame, row_index: int) -> int:
        """
        è®¡ç®—æŒ‡å®šè¡Œçš„åˆå¹¶å•å…ƒæ ¼æ•°é‡
        
        Args:
            df: DataFrame
            row_index: è¡Œç´¢å¼•
            
        Returns:
            åˆå¹¶å•å…ƒæ ¼æ•°é‡
        """
        try:
            # æ£€æŸ¥è¯¥è¡Œæ˜¯å¦æœ‰åˆå¹¶å•å…ƒæ ¼
            # é€šè¿‡æ£€æŸ¥è¿ç»­ç›¸åŒå€¼æ¥ä¼°ç®—åˆå¹¶å•å…ƒæ ¼
            row = df.iloc[row_index]
            merged_count = 0
            
            # æ£€æŸ¥è¿ç»­ç›¸åŒå€¼ï¼ˆå¯èƒ½æ˜¯åˆå¹¶å•å…ƒæ ¼çš„ç»“æœï¼‰
            current_value = None
            current_count = 0
            
            for value in row:
                if pd.isna(value):
                    if current_count > 1:
                        merged_count += current_count - 1
                    current_count = 0
                    current_value = None
                elif value == current_value:
                    current_count += 1
                else:
                    if current_count > 1:
                        merged_count += current_count - 1
                    current_count = 1
                    current_value = value
            
            # å¤„ç†æœ€åä¸€ç»„
            if current_count > 1:
                merged_count += current_count - 1
            
            return merged_count
        except Exception as e:
            print(f"  âš ï¸ è®¡ç®—åˆå¹¶å•å…ƒæ ¼æ•°é‡å¤±è´¥: {e}")
            return 0
    
    async def _is_empty_sheet(self, file_path: str, sheet_name: str) -> bool:
        """
        æ£€æŸ¥å·¥ä½œè¡¨æ˜¯å¦ä¸ºç©º
        
        Args:
            file_path: Excelæ–‡ä»¶è·¯å¾„
            sheet_name: å·¥ä½œè¡¨åç§°
            
        Returns:
            æ˜¯å¦ä¸ºç©ºå·¥ä½œè¡¨
        """
        try:
            # è¯»å–å·¥ä½œè¡¨
            df = pd.read_excel(file_path, sheet_name=sheet_name, header=None)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
            if len(df) == 0:
                return True
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å•å…ƒæ ¼éƒ½æ˜¯NaN
            if df.isnull().all().all():
                return True
            
            # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„éç©ºæ•°æ®ï¼ˆè‡³å°‘3è¡Œ3åˆ—ï¼‰
            non_null_count = df.notna().sum().sum()
            if non_null_count < 9:  # 3x3 = 9ä¸ªå•å…ƒæ ¼
                return True
            
            return False
            
        except Exception as e:
            print(f"âš ï¸ æ£€æŸ¥ç©ºå·¥ä½œè¡¨å¤±è´¥: {e}")
            return False
    
    async def _get_main_data_section(self, df: pd.DataFrame, sheet_name: str) -> pd.DataFrame:
        """
        è·å–ä¸»è¦æ•°æ®éƒ¨åˆ†ï¼Œåˆ é™¤è¡¨æœ«å°¾çš„æ— å…³éƒ¨åˆ†
        
        Args:
            df: åŸå§‹DataFrame
            sheet_name: å·¥ä½œè¡¨åç§°
            
        Returns:
            ä¸»è¦æ•°æ®éƒ¨åˆ†çš„DataFrame
        """
        try:
            print(f"ğŸ” å¼€å§‹åˆ†æä¸»è¦æ•°æ®éƒ¨åˆ†: {sheet_name}")
            
            # ä½¿ç”¨GPT-4åˆ†æä¸»è¦æ•°æ®éƒ¨åˆ†
            sample_text = df.head(50).to_string()
            
            prompt = f"""
è¯·åˆ†æä»¥ä¸‹Excelæ•°æ®ï¼Œæ‰¾å‡ºä¸»è¦æ•°æ®éƒ¨åˆ†çš„ç»“æŸä½ç½®ã€‚

æ•°æ®å†…å®¹ï¼š
{sample_text}

è¯·åˆ†æï¼š
1. ä¸»è¦æ•°æ®éƒ¨åˆ†ä»ç¬¬å‡ è¡Œå¼€å§‹åˆ°ç¬¬å‡ è¡Œç»“æŸï¼Ÿ
2. å“ªäº›è¡Œæ˜¯æ— å…³çš„ç»Ÿè®¡ä¿¡æ¯ã€æ±‡æ€»æ•°æ®æˆ–æ³¨é‡Šï¼Ÿ
3. ä¸»è¦æ•°æ®éƒ¨åˆ†åº”è¯¥åŒ…å«å“ªäº›è¡Œï¼Ÿ

è¯·ä»¥JSONæ ¼å¼è¿”å›ç»“æœï¼š
{{
    "main_data_start": èµ·å§‹è¡Œå·,
    "main_data_end": ç»“æŸè¡Œå·,
    "reason": "åˆ†æåŸå› "
}}

æ³¨æ„ï¼š
- è¡Œå·ä»0å¼€å§‹è®¡ç®—
- ä¸»è¦æ•°æ®éƒ¨åˆ†åº”è¯¥æ˜¯ç»“æ„ä¸€è‡´çš„æ•°æ®è¡Œ
- æ’é™¤è¡¨å¤´ã€ç»Ÿè®¡ä¿¡æ¯ã€æ±‡æ€»æ•°æ®ç­‰
"""
            
            response = await openai_client.call_gpt4(prompt)
            result = json.loads(response)
            
            start_row = result.get("main_data_start", 0)
            end_row = result.get("main_data_end", len(df))
            reason = result.get("reason", "æœªæŒ‡å®šåŸå› ")
            
            print(f"ğŸ“Š ä¸»è¦æ•°æ®éƒ¨åˆ†: ç¬¬{start_row}è¡Œåˆ°ç¬¬{end_row}è¡Œ")
            print(f"ğŸ“‹ åˆ†æåŸå› : {reason}")
            
            # æå–ä¸»è¦æ•°æ®éƒ¨åˆ†
            if end_row > start_row:
                main_data = df.iloc[start_row:end_row].copy()
                print(f"âœ… æå–ä¸»è¦æ•°æ®: {len(main_data)}è¡Œ Ã— {len(main_data.columns)}åˆ—")
                return main_data
            else:
                print(f"âš ï¸ ä¸»è¦æ•°æ®éƒ¨åˆ†æ— æ•ˆï¼Œè¿”å›åŸå§‹æ•°æ®")
                return df
                
        except Exception as e:
            print(f"âŒ è·å–ä¸»è¦æ•°æ®éƒ¨åˆ†å¤±è´¥: {e}")
            # å¦‚æœåˆ†æå¤±è´¥ï¼Œè¿”å›å‰80%çš„æ•°æ®
            end_row = int(len(df) * 0.8)
            print(f"ğŸ”„ ä½¿ç”¨é»˜è®¤ç­–ç•¥: è¿”å›å‰{end_row}è¡Œæ•°æ®")
            return df.iloc[:end_row].copy()
    
    async def _process_single_sheet_data_from_df(self, df: pd.DataFrame, sheet_name: str) -> pd.DataFrame:
        """
        ä»DataFrameå¤„ç†å•ä¸ªå·¥ä½œè¡¨æ•°æ®ï¼ŒåŒ…æ‹¬åˆå¹¶å•å…ƒæ ¼å’Œå¤šçº§è¡¨å¤´
        
        Args:
            df: è¾“å…¥DataFrame
            sheet_name: å·¥ä½œè¡¨åç§°
            
        Returns:
            å¤„ç†åçš„DataFrame
        """
        try:
            print(f"ğŸ”§ å¤„ç†å•å·¥ä½œè¡¨æ•°æ®: {sheet_name}")
            
            # æ£€æµ‹æ ‡ç­¾è¡Œ
            label_rows = self._detect_label_rows(df)
            print(f"ğŸ” æ£€æµ‹åˆ°æ ‡ç­¾è¡Œ: {label_rows}")
            
            if label_rows:
                # ç§»é™¤æ ‡ç­¾è¡Œ
                df = df.drop(label_rows).reset_index(drop=True)
                print(f"ğŸ—‘ï¸ å·²ç§»é™¤æ ‡ç­¾è¡Œ: {label_rows}")
            
            # æ£€æŸ¥å‰©ä½™æ•°æ®æ˜¯å¦è¶³å¤Ÿ
            if len(df) >= 2:
                # æ£€æµ‹å¤šçº§è¡¨å¤´
                print(f"ğŸ” æ£€æŸ¥å¤šçº§è¡¨å¤´:")
                print(f"  ç¬¬ä¸€è¡Œ: {df.iloc[0].tolist()}")
                if len(df) > 1:
                    print(f"  ç¬¬äºŒè¡Œ: {df.iloc[1].tolist()}")
                
                # ä½¿ç”¨GPT-4æ£€æµ‹å¤šçº§è¡¨å¤´
                header_rows = await self._detect_multi_level_headers(df, 0)
                print(f"ğŸ” å¼€å§‹å¤šçº§è¡¨å¤´æ£€æµ‹: è¡¨å¤´è¡Œ 0")
                print(f"multi_level_header_detection_start å¼€å§‹å¤šçº§è¡¨å¤´æ£€æµ‹: è¡¨å¤´è¡Œ 0")
                
                if len(header_rows) > 1:
                    print(f"ğŸ“Š å¤šçº§è¡¨å¤´æ£€æµ‹ç»“æœ: æ˜¯å¤šçº§è¡¨å¤´")
                    print(f"ğŸ“‹ è¡¨å¤´è¡Œ: {header_rows}")
                    print(f"multi_level_header_detection_complete å¤šçº§è¡¨å¤´æ£€æµ‹å®Œæˆ: æ˜¯å¤šçº§è¡¨å¤´, è¡¨å¤´è¡Œ: {header_rows}")
                    
                    # åˆå¹¶å¤šçº§è¡¨å¤´
                    merged_headers = self._merge_multi_level_headers(df, header_rows)
                    df.columns = merged_headers
                    print(f"ğŸ“‹ åˆå¹¶åçš„è¡¨å¤´: {df.columns.tolist()}")
                    print(f"multi_level_header_merged åˆå¹¶åçš„è¡¨å¤´: {df.columns.tolist()}")
                    
                    # è·³è¿‡è¡¨å¤´è¡Œ
                    df = df.iloc[len(header_rows):].reset_index(drop=True)
                    print(f"ğŸ“Š è·³è¿‡è¡¨å¤´è¡Œå: {len(df)}è¡Œ")
                else:
                    print(f"ğŸ“Š å¤šçº§è¡¨å¤´æ£€æµ‹ç»“æœ: æ˜¯å•çº§è¡¨å¤´")
                    print(f"ğŸ“‹ è¡¨å¤´è¡Œ: {header_rows}")
                    print(f"multi_level_header_detection_complete å¤šçº§è¡¨å¤´æ£€æµ‹å®Œæˆ: æ˜¯å•çº§è¡¨å¤´, è¡¨å¤´è¡Œ: {header_rows}")
                    print(f"ğŸ“‹ ä½¿ç”¨å•çº§è¡¨å¤´")
                    
                    # ä½¿ç”¨ç¬¬ä¸€è¡Œä½œä¸ºåˆ—å
                    df.columns = [str(col).strip() if pd.notna(col) and str(col).strip() else f"Column_{i}"
                                 for i, col in enumerate(df.iloc[0])]
                    df = df.iloc[1:].reset_index(drop=True)
                    print(f"ğŸ“‹ åˆ—å: {df.columns.tolist()}")
            else:
                print(f"âš ï¸ æ•°æ®è¡Œæ•°ä¸è¶³ï¼Œä½¿ç”¨é»˜è®¤åˆ—å")
                df.columns = [f"Column_{i}" for i in range(len(df.columns))]
            
            print(f"âœ… å•å·¥ä½œè¡¨æ•°æ®å¤„ç†å®Œæˆ: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
            return df
            
        except Exception as e:
            print(f"âŒ å¤„ç†å•å·¥ä½œè¡¨æ•°æ®å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return df
    
    async def _identify_irrelevant_rows(self, file_path: str, sheet_name: str = None) -> Tuple[List[int], int]:
        """
        ä½¿ç”¨GPT-4è¯†åˆ«æ— å…³è¡Œ
        
        Args:
            file_path: Excelæ–‡ä»¶è·¯å¾„
            sheet_name: å·¥ä½œè¡¨åç§°ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            (è¦è·³è¿‡çš„è¡Œå·åˆ—è¡¨, è¡¨å¤´è¡Œå·)
        """
        # è¯»å–å‰20è¡Œ
        df_sample = pd.read_excel(file_path, sheet_name=sheet_name, header=None, nrows=20)
        sample_text = df_sample.to_string()
        
        prompt = f"""åˆ†æä»¥ä¸‹Excelæ–‡ä»¶çš„å‰20è¡Œæ•°æ®ï¼Œè¯†åˆ«ï¼š
1. å“ªäº›è¡Œæ˜¯æ ‡é¢˜ã€è¯´æ˜ã€ç©ºè¡Œç­‰éæ•°æ®å†…å®¹
2. å“ªä¸€è¡Œæ˜¯çœŸæ­£çš„æ•°æ®è¡¨å¤´

è¿”å›JSONæ ¼å¼ï¼š
{{
    "skip_rows": [è¦è·³è¿‡çš„è¡Œå·åˆ—è¡¨ï¼Œä»0å¼€å§‹],
    "header_row": è¡¨å¤´è¡Œå·ï¼ˆä»0å¼€å§‹ï¼‰
}}

æ•°æ®æ ·æœ¬ï¼š
{sample_text}

è¯·ä»”ç»†åˆ†æï¼Œè¿”å›çº¯JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""
        
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªExcelæ•°æ®åˆ†æä¸“å®¶ï¼Œæ“…é•¿è¯†åˆ«è¡¨æ ¼ç»“æ„ã€‚"},
            {"role": "user", "content": prompt}
        ]
        
        try:
            response = await openai_client.chat_completion(messages, temperature=0.1)
            result = openai_client.extract_json(response)
            
            skip_rows = result.get("skip_rows", [])
            header_row = result.get("header_row", 0)
            
            return skip_rows, header_row
        except Exception as e:
            print(f"GPT-4è¯†åˆ«å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥: {str(e)}")
            # é»˜è®¤ï¼šè·³è¿‡å‰é¢çš„ç©ºè¡Œ
            return [], 0
    
    def _unmerge_cells(self, worksheet):
        """æ‹†åˆ†åˆå¹¶å•å…ƒæ ¼å¹¶å¡«å……å€¼"""
        merged_ranges = list(worksheet.merged_cells.ranges)
        
        for merged_range in merged_ranges:
            # è·å–åˆå¹¶åŒºåŸŸå·¦ä¸Šè§’çš„å€¼
            min_col, min_row, max_col, max_row = merged_range.bounds
            value = worksheet.cell(min_row, min_col).value
            
            # æ‹†åˆ†åˆå¹¶å•å…ƒæ ¼
            worksheet.unmerge_cells(str(merged_range))
            
            # å¡«å……æ‰€æœ‰å•å…ƒæ ¼
            for row in range(min_row, max_row + 1):
                for col in range(min_col, max_col + 1):
                    worksheet.cell(row, col).value = value
    
    def _merge_multi_level_headers(self, worksheet, header_start_row: int) -> List[str]:
        """
        åˆå¹¶å¤šçº§è¡¨å¤´
        
        Args:
            worksheet: openpyxlå·¥ä½œè¡¨
            header_start_row: è¡¨å¤´èµ·å§‹è¡Œï¼ˆä»0å¼€å§‹ï¼Œä½†openpyxlä»1å¼€å§‹ï¼‰
            
        Returns:
            åˆå¹¶åçš„åˆ—ååˆ—è¡¨
        """
        # æ£€æµ‹è¡¨å¤´å±‚æ•°ï¼ˆè¿ç»­çš„éç©ºè¡Œï¼‰
        header_rows = []
        row_idx = header_start_row + 1  # openpyxlä»1å¼€å§‹
        
        while row_idx <= min(row_idx + 5, worksheet.max_row):
            row_values = [cell.value for cell in worksheet[row_idx]]
            # å¦‚æœå¤§éƒ¨åˆ†æ˜¯ç©ºå€¼ï¼Œè®¤ä¸ºè¡¨å¤´ç»“æŸ
            if sum(1 for v in row_values if v is not None) < len(row_values) * 0.3:
                break
            header_rows.append(row_values)
            row_idx += 1
        
        if not header_rows:
            return []
        
        # å¦‚æœåªæœ‰ä¸€å±‚è¡¨å¤´
        if len(header_rows) == 1:
            return [str(v) if v is not None else f"Column_{i}" for i, v in enumerate(header_rows[0])]
        
        # å¤šçº§è¡¨å¤´åˆå¹¶
        column_names = []
        num_cols = len(header_rows[0])
        
        for col_idx in range(num_cols):
            parts = []
            for row in header_rows:
                if col_idx < len(row) and row[col_idx] is not None:
                    val = str(row[col_idx]).strip()
                    if val and val not in parts:  # é¿å…é‡å¤
                        parts.append(val)
            
            if parts:
                column_names.append("_".join(parts))
            else:
                column_names.append(f"Column_{col_idx}")
        
        return column_names
    
    async def _check_schema_split_needed(self, file_path: str, sheet_name: str) -> bool:
        """
        æ£€æŸ¥å·¥ä½œè¡¨æ˜¯å¦éœ€è¦Schemaåˆ†å‰²
        
        Args:
            file_path: Excelæ–‡ä»¶è·¯å¾„
            sheet_name: å·¥ä½œè¡¨åç§°
            
        Returns:
            æ˜¯å¦éœ€è¦Schemaåˆ†å‰²
        """
        try:
            print(f"ğŸ” å¼€å§‹Schemaåˆ†å‰²æ£€æŸ¥: {sheet_name}")
            print(f"å¼€å§‹Schemaåˆ†å‰²æ£€æŸ¥: {sheet_name}")
            
            # è¯»å–æ•´ä¸ªå·¥ä½œè¡¨
            df = pd.read_excel(file_path, sheet_name=sheet_name, header=None)
            print(f"ğŸ“Š è¯»å–æ•°æ®: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
            print(f"è¯»å–æ•°æ®: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
            
            # å¦‚æœæ•°æ®å¤ªå°‘ï¼Œä¸éœ€è¦åˆ†å‰²
            if len(df) < 10:
                print(f"ğŸ“Š æ•°æ®é‡ä¸è¶³ï¼Œè·³è¿‡Schemaåˆ†å‰²æ£€æŸ¥: {len(df)}è¡Œ")
                print(f"schema_check_skipped æ•°æ®é‡ä¸è¶³ï¼Œè·³è¿‡Schemaåˆ†å‰²æ£€æŸ¥: {len(df)}è¡Œ")
                return False
            
            # ä½¿ç”¨GPT-4åˆ†ææ˜¯å¦éœ€è¦Schemaåˆ†å‰²
            # é™åˆ¶æ ·æœ¬æ•°æ®é•¿åº¦ï¼Œé¿å…è¶…è¿‡tokené™åˆ¶
            sample_df = df.head(20)  # å‡å°‘åˆ°20è¡Œ
            sample_text = sample_df.to_string(max_rows=20, max_cols=10)  # é™åˆ¶åˆ—æ•°
            
            # å¦‚æœæ–‡æœ¬å¤ªé•¿ï¼Œè¿›ä¸€æ­¥æˆªæ–­
            if len(sample_text) > 1500:  # é™ä½åˆ°1500å­—ç¬¦ï¼Œç•™å‡ºæ›´å¤šä½™é‡
                sample_text = sample_text[:1497] + "..."  # 1497 + 3 = 1500
            
            print(f"ğŸ¤– è°ƒç”¨GPT-4è¿›è¡ŒSchemaåˆ†æ...")
            print(f"gpt_analysis_start è°ƒç”¨GPT-4è¿›è¡ŒSchemaåˆ†æ: {sheet_name}")
            print(f"ğŸ“Š æ ·æœ¬æ•°æ®é•¿åº¦: {len(sample_text)} å­—ç¬¦")
            
            prompt = f"""åˆ†æä»¥ä¸‹Excelå·¥ä½œè¡¨æ•°æ®ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦æŒ‰Schemaåˆ†å‰²ï¼š

1. æ£€æŸ¥æ•°æ®æ˜¯å¦åŒ…å«å¤šä¸ªä¸åŒçš„æ•°æ®ç»“æ„ï¼ˆå¦‚ï¼šç»Ÿè®¡ä¿¡æ¯ã€è¯¦ç»†æ•°æ®ã€æ±‡æ€»è¡¨ç­‰ï¼‰
2. æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çš„åˆ†éš”è¡Œï¼ˆå¦‚ï¼šç©ºè¡Œã€æ ‡é¢˜è¡Œã€ç»Ÿè®¡å€¼è¡Œç­‰ï¼‰
3. æ£€æŸ¥åˆ—ç»“æ„æ˜¯å¦åœ¨ä¸åŒåŒºåŸŸå‘ç”Ÿå˜åŒ–

è¿”å›JSONæ ¼å¼ï¼š
{{
    "needs_split": true/false,
    "reason": "åˆ†å‰²åŸå› è¯´æ˜",
    "split_points": [åˆ†å‰²ç‚¹è¡Œå·åˆ—è¡¨ï¼Œä»0å¼€å§‹],
    "schema_descriptions": ["å„Schemaçš„æè¿°"]
}}

æ•°æ®æ ·æœ¬ï¼ˆå‰20è¡Œï¼‰ï¼š
{sample_text}

è¯·ä»”ç»†åˆ†æï¼Œè¿”å›çº¯JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""
            
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œæ“…é•¿è¯†åˆ«Excelè¡¨æ ¼ä¸­çš„æ•°æ®ç»“æ„å˜åŒ–å’ŒSchemaåˆ†å‰²ç‚¹ã€‚"},
                {"role": "user", "content": prompt}
            ]
            
            response = await openai_client.chat_completion(messages)
            result = openai_client.extract_json(response)
            
            needs_split = result.get("needs_split", False)
            reason = result.get("reason", "æœªçŸ¥åŸå› ")
            
            print(f"ğŸ“Š Schemaåˆ†å‰²åˆ†æç»“æœ: {'éœ€è¦åˆ†å‰²' if needs_split else 'ä¸éœ€è¦åˆ†å‰²'}")
            if needs_split:
                print(f"ğŸ“‹ åˆ†å‰²åŸå› : {reason}")
                split_points = result.get("split_points", [])
                schema_descriptions = result.get("schema_descriptions", [])
                print(f"ğŸ“ åˆ†å‰²ç‚¹: {split_points}")
                print(f"ğŸ“ Schemaæè¿°: {schema_descriptions}")
            
            print(f"schema_check_complete Schemaåˆ†å‰²æ£€æŸ¥å®Œæˆ: {'éœ€è¦åˆ†å‰²' if needs_split else 'ä¸éœ€è¦åˆ†å‰²'}, åŸå› : {reason}")
            
            return needs_split
            
        except Exception as e:
            print(f"âŒ Schemaåˆ†å‰²æ£€æŸ¥å¤±è´¥: {e}")
            logger.log_error("schema_check_failed", f"Schemaåˆ†å‰²æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    async def _process_schema_split_sheet(self, file_path: str, sheet_name: str) -> Dict[str, Dict[str, Any]]:
        """
        å¤„ç†éœ€è¦Schemaåˆ†å‰²çš„å·¥ä½œè¡¨
        
        Args:
            file_path: Excelæ–‡ä»¶è·¯å¾„
            sheet_name: å·¥ä½œè¡¨åç§°
            
        Returns:
            åˆ†å‰²åçš„Schemaæ•°æ®å­—å…¸
        """
        try:
            print(f"ğŸ” å¼€å§‹Schemaåˆ†å‰²å¤„ç†: {sheet_name}")
            print(f"schema_split_processing_start å¼€å§‹Schemaåˆ†å‰²å¤„ç†: {sheet_name}")
            
            # å…ˆå¤„ç†å…¬å¼è½¬å€¼
            wb = openpyxl.load_workbook(file_path)
            ws = wb[sheet_name]
            self._convert_formulas_to_values(ws)
            
            # ä¿å­˜å¤„ç†åçš„ä¸´æ—¶æ–‡ä»¶
            temp_file = file_path.replace('.xlsx', f'_temp_schema_{sheet_name}.xlsx')
            wb.save(temp_file)
            print(f"ğŸ’¾ ä¿å­˜Schemaå¤„ç†ä¸´æ—¶æ–‡ä»¶: {temp_file}")
            
            try:
                # è¯»å–æ•´ä¸ªå·¥ä½œè¡¨
                df = pd.read_excel(temp_file, sheet_name=sheet_name, header=None)
            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                Path(temp_file).unlink(missing_ok=True)
                print(f"ğŸ—‘ï¸ æ¸…ç†Schemaå¤„ç†ä¸´æ—¶æ–‡ä»¶: {temp_file}")
            print(f"ğŸ“Š è¯»å–å®Œæ•´æ•°æ®: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
            print(f"full_data_loaded è¯»å–å®Œæ•´æ•°æ®: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
            
            # ä½¿ç”¨GPT-4åˆ†æåˆ†å‰²ç‚¹
            sample_text = df.to_string()
            print(f"ğŸ¤– è°ƒç”¨GPT-4è¿›è¡ŒSchemaåˆ†å‰²åˆ†æ...")
            print(f"gpt_schema_analysis_start è°ƒç”¨GPT-4è¿›è¡ŒSchemaåˆ†å‰²åˆ†æ: {sheet_name}")
            
            prompt = f"""åˆ†æä»¥ä¸‹Excelå·¥ä½œè¡¨ï¼Œè¯†åˆ«Schemaåˆ†å‰²ç‚¹ï¼š

1. æ‰¾å‡ºæ•°æ®ç»“æ„å‘ç”Ÿå˜åŒ–çš„åˆ†å‰²ç‚¹
2. ä¸ºæ¯ä¸ªSchemaåŒºåŸŸæä¾›æè¿°
3. è¯†åˆ«æ¯ä¸ªåŒºåŸŸçš„è¡¨å¤´ä½ç½®

è¿”å›JSONæ ¼å¼ï¼š
{{
    "schema_regions": [
        {{
            "start_row": èµ·å§‹è¡Œå·ï¼ˆä»0å¼€å§‹ï¼‰,
            "end_row": ç»“æŸè¡Œå·ï¼ˆä»0å¼€å§‹ï¼Œ-1è¡¨ç¤ºåˆ°æœ€åï¼‰,
            "header_row": è¡¨å¤´è¡Œå·ï¼ˆä»0å¼€å§‹ï¼‰,
            "description": "Schemaæè¿°",
            "schema_name": "Schemaåç§°"
        }}
    ]
}}

æ•°æ®ï¼š
{sample_text}

è¯·ä»”ç»†åˆ†æï¼Œè¿”å›çº¯JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""
            
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œæ“…é•¿è¯†åˆ«Excelè¡¨æ ¼ä¸­çš„Schemaåˆ†å‰²ç‚¹å’Œæ•°æ®ç»“æ„ã€‚"},
                {"role": "user", "content": prompt}
            ]
            
            response = await openai_client.chat_completion(messages)
            result = openai_client.extract_json(response)
            
            schema_sheets = {}
            schema_regions = result.get("schema_regions", [])
            print(f"ğŸ“‹ æ£€æµ‹åˆ° {len(schema_regions)} ä¸ªSchemaåŒºåŸŸ")
            print(f"schema_regions_detected æ£€æµ‹åˆ° {len(schema_regions)} ä¸ªSchemaåŒºåŸŸ")
            
            for i, region in enumerate(schema_regions):
                start_row = region.get("start_row", 0)
                end_row = region.get("end_row", -1)
                header_row = region.get("header_row", start_row)
                description = region.get("description", "æœªçŸ¥Schema")
                schema_name = region.get("schema_name", f"{sheet_name}_{description}")
                
                print(f"ğŸ“‹ å¤„ç†SchemaåŒºåŸŸ {i+1}: {schema_name}")
                print(f"  - æè¿°: {description}")
                print(f"  - èµ·å§‹è¡Œ: {start_row}, ç»“æŸè¡Œ: {end_row}, è¡¨å¤´è¡Œ: {header_row}")
                print(f"schema_region_processing å¤„ç†SchemaåŒºåŸŸ: {schema_name}, æè¿°: {description}")
                
                # æå–Schemaæ•°æ®
                if end_row == -1:
                    schema_df = df.iloc[start_row:].copy()
                else:
                    schema_df = df.iloc[start_row:end_row+1].copy()
                
                print(f"  - æå–æ•°æ®: {len(schema_df)}è¡Œ Ã— {len(schema_df.columns)}åˆ—")
                
                schema_sheets[schema_name] = {
                    "data": schema_df,
                    "header_row": header_row - start_row,  # ç›¸å¯¹äºSchemaçš„headerä½ç½®
                    "description": description,
                    "original_sheet": sheet_name
                }
            
            print(f"âœ… Schemaåˆ†å‰²å¤„ç†å®Œæˆ: {len(schema_sheets)}ä¸ªSchema")
            print(f"schema_split_processing_complete Schemaåˆ†å‰²å¤„ç†å®Œæˆ: {len(schema_sheets)}ä¸ªSchema")
            
            return schema_sheets
            
        except Exception as e:
            print(f"âŒ Schemaåˆ†å‰²å¤„ç†å¤±è´¥: {e}")
            logger.log_error("schema_split_processing_failed", f"Schemaåˆ†å‰²å¤„ç†å¤±è´¥: {e}")
            return {}
    
    async def _process_schema_data(self, schema_data: Dict[str, Any]) -> pd.DataFrame:
        """
        å¤„ç†å•ä¸ªSchemaçš„æ•°æ®
        
        Args:
            schema_data: Schemaæ•°æ®å­—å…¸
            
        Returns:
            å¤„ç†åçš„DataFrame
        """
        schema_name = schema_data.get("description", "æœªçŸ¥Schema")
        print(f"ğŸ”„ å¼€å§‹å¤„ç†Schemaæ•°æ®: {schema_name}")
        print(f"schema_data_processing_start å¼€å§‹å¤„ç†Schemaæ•°æ®: {schema_name}")
        
        df = schema_data["data"].copy()
        header_row = schema_data["header_row"]
        original_sheet = schema_data["original_sheet"]
        
        print(f"ğŸ“Š åŸå§‹æ•°æ®: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—, è¡¨å¤´è¡Œ: {header_row}")
        print(f"schema_data_info åŸå§‹æ•°æ®: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—, è¡¨å¤´è¡Œ: {header_row}")
        
        # 1. å¤„ç†åˆå¹¶å•å…ƒæ ¼
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦ä»åŸå§‹æ–‡ä»¶è·¯å¾„é‡æ–°åŠ è½½ï¼Œè€Œä¸æ˜¯ä½¿ç”¨sheet_name
        # ç”±äºæˆ‘ä»¬å·²ç»åœ¨ä¸»æ–¹æ³•ä¸­å¤„ç†äº†åˆå¹¶å•å…ƒæ ¼ï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨DataFrame
        print(f"ğŸ”§ åˆå¹¶å•å…ƒæ ¼å·²åœ¨ä¸»æ–¹æ³•ä¸­å¤„ç†")
        
        # 2. å¤„ç†å¤šçº§è¡¨å¤´
        if header_row >= 0 and header_row < len(df):
            print(f"ğŸ” æ£€æŸ¥å¤šçº§è¡¨å¤´: è¡¨å¤´è¡Œ {header_row}")
            print(f"multi_level_header_check æ£€æŸ¥å¤šçº§è¡¨å¤´: è¡¨å¤´è¡Œ {header_row}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¤šçº§è¡¨å¤´
            multi_level_headers = await self._detect_multi_level_headers(df, header_row)
            print(f"ğŸ“Š å¤šçº§è¡¨å¤´æ£€æµ‹ç»“æœ: {len(multi_level_headers)}ä¸ªè¡¨å¤´è¡Œ {multi_level_headers}")
            print(f"multi_level_header_detected å¤šçº§è¡¨å¤´æ£€æµ‹ç»“æœ: {len(multi_level_headers)}ä¸ªè¡¨å¤´è¡Œ {multi_level_headers}")
            
            if len(multi_level_headers) > 1:
                print(f"ğŸ”— åˆå¹¶å¤šçº§è¡¨å¤´...")
                # åˆå¹¶å¤šçº§è¡¨å¤´
                merged_headers = self._merge_multi_level_headers(df, multi_level_headers)
                print(f"ğŸ“‹ åˆå¹¶åçš„è¡¨å¤´: {merged_headers}")
                print(f"multi_level_header_merged åˆå¹¶åçš„è¡¨å¤´: {merged_headers}")
                
                df.columns = merged_headers
                # è·³è¿‡æ‰€æœ‰è¡¨å¤´è¡Œ
                df = df.iloc[max(multi_level_headers)+1:].reset_index(drop=True)
                print(f"ğŸ“Š è·³è¿‡è¡¨å¤´è¡Œå: {len(df)}è¡Œ")
            else:
                print(f"ğŸ“‹ ä½¿ç”¨å•çº§è¡¨å¤´...")
                # ä½¿ç”¨å•çº§è¡¨å¤´
                header_values = df.iloc[header_row].tolist()
                print(f"ğŸ“‹ åŸå§‹è¡¨å¤´å€¼: {header_values}")
                
                # ç¡®ä¿åˆ—åæ˜¯å­—ç¬¦ä¸²ä¸”ä¸ä¸ºç©º
                clean_headers = []
                for i, val in enumerate(header_values):
                    if pd.notna(val) and str(val).strip():
                        clean_headers.append(str(val).strip())
                    else:
                        clean_headers.append(f"Column_{i}")
                
                print(f"ğŸ“‹ æ¸…ç†åçš„è¡¨å¤´: {clean_headers}")
                print(f"single_level_header_processed æ¸…ç†åçš„è¡¨å¤´: {clean_headers}")
                
                df.columns = clean_headers
                df = df.iloc[header_row+1:].reset_index(drop=True)
                print(f"ğŸ“Š è·³è¿‡è¡¨å¤´è¡Œå: {len(df)}è¡Œ")
        else:
            print(f"âš ï¸ è¡¨å¤´è¡Œæ— æ•ˆï¼Œä½¿ç”¨é»˜è®¤åˆ—å")
            print(f"invalid_header_row è¡¨å¤´è¡Œæ— æ•ˆ: {header_row}, ä½¿ç”¨é»˜è®¤åˆ—å")
        
        # 3. æ¸…ç†æ•°æ®
        print(f"ğŸ§¹ å¼€å§‹æ•°æ®æ¸…ç†...")
        print(f"data_cleaning_start å¼€å§‹æ•°æ®æ¸…ç†: {schema_name}")
        
        original_rows = len(df)
        original_cols = len(df.columns)
        
        df = self._clean_dataframe(df)
        
        print(f"âœ… æ•°æ®æ¸…ç†å®Œæˆ: {original_rows}è¡Œ â†’ {len(df)}è¡Œ, {original_cols}åˆ— â†’ {len(df.columns)}åˆ—")
        print(f"data_cleaning_complete æ•°æ®æ¸…ç†å®Œæˆ: {original_rows}è¡Œ â†’ {len(df)}è¡Œ, {original_cols}åˆ— â†’ {len(df.columns)}åˆ—")
        
        print(f"âœ… Schemaæ•°æ®å¤„ç†å®Œæˆ: {schema_name}")
        print(f"schema_data_processing_complete Schemaæ•°æ®å¤„ç†å®Œæˆ: {schema_name}")
        
        return df
    
    def _unmerge_cells_with_fill(self, worksheet):
        """
        æ‹†åˆ†åˆå¹¶å•å…ƒæ ¼å¹¶å¡«å……å€¼åˆ°æ‰€æœ‰å•å…ƒæ ¼
        
        Args:
            worksheet: openpyxlå·¥ä½œè¡¨
        """
        merged_ranges = list(worksheet.merged_cells.ranges)
        
        for merged_range in merged_ranges:
            # è·å–åˆå¹¶åŒºåŸŸå·¦ä¸Šè§’çš„å€¼
            min_col, min_row, max_col, max_row = merged_range.bounds
            top_left_cell_value = worksheet.cell(min_row, min_col).value
            
            # æ‹†åˆ†åˆå¹¶å•å…ƒæ ¼
            worksheet.unmerge_cells(str(merged_range))
            
            # ç”¨åŸå€¼å¡«å……æ‰€æœ‰å•å…ƒæ ¼
            for row in range(min_row, max_row + 1):
                for col in range(min_col, max_col + 1):
                    worksheet.cell(row, col).value = top_left_cell_value
    
    async def _detect_multi_level_headers(self, df: pd.DataFrame, header_row: int) -> List[int]:
        """
        æ£€æµ‹å¤šçº§è¡¨å¤´
        
        Args:
            df: DataFrame
            header_row: è¡¨å¤´è¡Œå·
            
        Returns:
            å¤šçº§è¡¨å¤´çš„è¡Œå·åˆ—è¡¨
        """
        try:
            print(f"ğŸ” å¼€å§‹å¤šçº§è¡¨å¤´æ£€æµ‹: è¡¨å¤´è¡Œ {header_row}")
            print(f"multi_level_header_detection_start å¼€å§‹å¤šçº§è¡¨å¤´æ£€æµ‹: è¡¨å¤´è¡Œ {header_row}")
            
            # æ£€æŸ¥è¡¨å¤´è¡Œé™„è¿‘çš„è¡Œ
            check_rows = max(0, header_row - 2)
            check_end = min(len(df), header_row + 3)
            
            print(f"ğŸ“Š æ£€æŸ¥è¡ŒèŒƒå›´: {check_rows} åˆ° {check_end}")
            print(f"header_check_range æ£€æŸ¥è¡ŒèŒƒå›´: {check_rows} åˆ° {check_end}")
            
            sample_data = df.iloc[check_rows:check_end].to_string()
            
            print(f"ğŸ¤– è°ƒç”¨GPT-4è¿›è¡Œå¤šçº§è¡¨å¤´åˆ†æ...")
            print(f"gpt_multi_level_analysis_start è°ƒç”¨GPT-4è¿›è¡Œå¤šçº§è¡¨å¤´åˆ†æ")
            
            prompt = f"""åˆ†æä»¥ä¸‹Excelæ•°æ®ï¼Œæ£€æµ‹å¤šçº§è¡¨å¤´ï¼š

1. æ£€æŸ¥è¡¨å¤´è¡Œé™„è¿‘æ˜¯å¦æœ‰å¤šä¸ªè¡¨å¤´è¡Œ
2. åˆ¤æ–­å“ªäº›è¡Œæ˜¯è¡¨å¤´çš„ä¸€éƒ¨åˆ†
3. è¯†åˆ«è¡¨å¤´çš„å±‚çº§ç»“æ„

è¿”å›JSONæ ¼å¼ï¼š
{{
    "is_multi_level": true/false,
    "header_rows": [è¡¨å¤´è¡Œå·åˆ—è¡¨ï¼Œä»0å¼€å§‹],
    "header_levels": {{
        "row_0": "ç¬¬ä¸€çº§è¡¨å¤´æè¿°",
        "row_1": "ç¬¬äºŒçº§è¡¨å¤´æè¿°"
    }}
}}

æ•°æ®æ ·æœ¬ï¼š
{sample_data}

è¯·ä»”ç»†åˆ†æï¼Œè¿”å›çº¯JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚"""
            
            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•°æ®åˆ†æå¸ˆï¼Œæ“…é•¿è¯†åˆ«Excelè¡¨æ ¼ä¸­çš„å¤šçº§è¡¨å¤´ç»“æ„ã€‚"},
                {"role": "user", "content": prompt}
            ]
            
            response = await openai_client.chat_completion(messages)
            result = openai_client.extract_json(response)
            
            is_multi_level = result.get("is_multi_level", False)
            header_rows = result.get("header_rows", [header_row])
            header_levels = result.get("header_levels", {})
            
            print(f"ğŸ“Š å¤šçº§è¡¨å¤´æ£€æµ‹ç»“æœ: {'æ˜¯å¤šçº§è¡¨å¤´' if is_multi_level else 'æ˜¯å•çº§è¡¨å¤´'}")
            print(f"ğŸ“‹ è¡¨å¤´è¡Œ: {header_rows}")
            if header_levels:
                print(f"ğŸ“ è¡¨å¤´å±‚çº§: {header_levels}")
            
            print(f"multi_level_header_detection_complete å¤šçº§è¡¨å¤´æ£€æµ‹å®Œæˆ: {'æ˜¯å¤šçº§è¡¨å¤´' if is_multi_level else 'æ˜¯å•çº§è¡¨å¤´'}, è¡¨å¤´è¡Œ: {header_rows}")
            
            if is_multi_level:
                return header_rows
            else:
                return [header_row]
                
        except Exception as e:
            print(f"âŒ å¤šçº§è¡¨å¤´æ£€æµ‹å¤±è´¥: {e}")
            logger.log_error("multi_level_header_detection_failed", f"å¤šçº§è¡¨å¤´æ£€æµ‹å¤±è´¥: {e}")
            return [header_row]
    
    def _merge_multi_level_headers(self, df: pd.DataFrame, header_rows: List[int]) -> List[str]:
        """
        åˆå¹¶å¤šçº§è¡¨å¤´ä¸ºå•çº§è¡¨å¤´
        
        Args:
            df: DataFrame
            header_rows: è¡¨å¤´è¡Œå·åˆ—è¡¨
            
        Returns:
            åˆå¹¶åçš„åˆ—ååˆ—è¡¨
        """
        print(f"ğŸ”— å¼€å§‹åˆå¹¶å¤šçº§è¡¨å¤´: {len(header_rows)}ä¸ªè¡¨å¤´è¡Œ")
        print(f"multi_level_header_merge_start å¼€å§‹åˆå¹¶å¤šçº§è¡¨å¤´: {len(header_rows)}ä¸ªè¡¨å¤´è¡Œ")
        
        merged_headers = []
        
        for col_idx in range(len(df.columns)):
            col_values = []
            
            # æ”¶é›†æ¯çº§è¡¨å¤´çš„å€¼
            for row_idx in header_rows:
                if row_idx < len(df):
                    value = df.iloc[row_idx, col_idx]
                    if pd.notna(value) and str(value).strip():
                        col_values.append(str(value).strip())
            
            # åˆå¹¶è¡¨å¤´å€¼
            if len(col_values) > 1:
                # å»é‡å¹¶åˆå¹¶
                unique_values = []
                for val in col_values:
                    if val not in unique_values:
                        unique_values.append(val)
                
                if len(unique_values) > 1:
                    # å¦‚æœå»é‡åä»æœ‰å¤šä¸ªå€¼ï¼Œç”¨"-"è¿æ¥
                    merged_header = "-".join(unique_values)
                    print(f"  ğŸ“‹ åˆ—{col_idx}: {' + '.join(col_values)} â†’ {merged_header}")
                else:
                    # å¦‚æœå»é‡ååªæœ‰ä¸€ä¸ªå€¼ï¼Œç›´æ¥ä½¿ç”¨
                    merged_header = unique_values[0]
                    print(f"  ğŸ“‹ åˆ—{col_idx}: {' + '.join(col_values)} â†’ {merged_header}")
            elif len(col_values) == 1:
                # å¦‚æœåªæœ‰ä¸€ä¸ªå€¼ï¼Œç›´æ¥ä½¿ç”¨
                merged_header = col_values[0]
                print(f"  ğŸ“‹ åˆ—{col_idx}: {merged_header}")
            else:
                # å¦‚æœæ²¡æœ‰å€¼ï¼Œä½¿ç”¨é»˜è®¤åç§°
                merged_header = f"Column_{col_idx}"
                print(f"  ğŸ“‹ åˆ—{col_idx}: ç©ºå€¼ â†’ {merged_header}")
            
            merged_headers.append(merged_header)
        
        print(f"âœ… å¤šçº§è¡¨å¤´åˆå¹¶å®Œæˆ: {len(merged_headers)}åˆ—")
        print(f"multi_level_header_merge_complete å¤šçº§è¡¨å¤´åˆå¹¶å®Œæˆ: {len(merged_headers)}åˆ—")
        
        return merged_headers
    
    def _write_dataframe_to_worksheet(self, worksheet, df: pd.DataFrame):
        """
        å°†DataFrameå†™å…¥å·¥ä½œè¡¨
        
        Args:
            worksheet: openpyxlå·¥ä½œè¡¨
            df: DataFrame
        """
        print(f"ğŸ“ å¼€å§‹å†™å…¥DataFrameåˆ°å·¥ä½œè¡¨: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
        print(f"dataframe_write_start å¼€å§‹å†™å…¥DataFrameåˆ°å·¥ä½œè¡¨: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
        
        # å†™å…¥è¡¨å¤´
        print(f"ğŸ“‹ å†™å…¥è¡¨å¤´: {list(df.columns)}")
        for col_idx, col_name in enumerate(df.columns, 1):
            worksheet.cell(row=1, column=col_idx, value=col_name)
        
        # å†™å…¥æ•°æ®
        print(f"ğŸ“Š å†™å…¥æ•°æ®è¡Œ...")
        for row_idx, row_data in enumerate(df.values, 2):
            for col_idx, value in enumerate(row_data, 1):
                worksheet.cell(row=row_idx, column=col_idx, value=value)
        
        print(f"âœ… DataFrameå†™å…¥å®Œæˆ")
        print(f"dataframe_write_complete DataFrameå†™å…¥å®Œæˆ: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
    
    def _clean_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…ç†DataFrame"""
        print(f"ğŸ§¹ å¼€å§‹æ¸…ç†DataFrame: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
        print(f"dataframe_cleaning_start å¼€å§‹æ¸…ç†DataFrame: {len(df)}è¡Œ Ã— {len(df.columns)}åˆ—")
        
        original_rows = len(df)
        original_cols = len(df.columns)
        
        # åˆ é™¤å…¨ç©ºè¡Œ
        df = df.dropna(how='all')
        empty_rows_removed = original_rows - len(df)
        if empty_rows_removed > 0:
            print(f"ğŸ—‘ï¸ åˆ é™¤ç©ºè¡Œ: {empty_rows_removed}è¡Œ")
            print(f"empty_rows_removed åˆ é™¤ç©ºè¡Œ: {empty_rows_removed}è¡Œ")
        
        # åˆ é™¤å…¨ç©ºåˆ—
        df = df.dropna(axis=1, how='all')
        empty_cols_removed = original_cols - len(df.columns)
        if empty_cols_removed > 0:
            print(f"ğŸ—‘ï¸ åˆ é™¤ç©ºåˆ—: {empty_cols_removed}åˆ—")
            print(f"empty_cols_removed åˆ é™¤ç©ºåˆ—: {empty_cols_removed}åˆ—")
        
        # ç¡®ä¿åˆ—åå”¯ä¸€
        print(f"ğŸ”§ å¤„ç†åˆ—åå”¯ä¸€æ€§...")
        cols = []
        renamed_cols = 0
        for col in df.columns:
            col_str = str(col) if col is not None else "Unnamed"
            original_col_str = col_str
            # å¦‚æœåˆ—åå·²å­˜åœ¨ï¼Œæ·»åŠ åç¼€
            if col_str in cols:
                i = 1
                while f"{col_str}_{i}" in cols:
                    i += 1
                col_str = f"{col_str}_{i}"
                if col_str != original_col_str:
                    renamed_cols += 1
            cols.append(col_str)
        
        if renamed_cols > 0:
            print(f"ğŸ“ é‡å‘½åé‡å¤åˆ—å: {renamed_cols}åˆ—")
            print(f"duplicate_cols_renamed é‡å‘½åé‡å¤åˆ—å: {renamed_cols}åˆ—")
        
        df.columns = cols
        
        # å¤„ç†NaNå€¼ï¼Œé¿å…Elasticsearchç´¢å¼•å¤±è´¥
        print(f"ğŸ”§ å¤„ç†NaNå€¼...")
        nan_count = 0
        for col in df.columns:
            # æ£€æŸ¥æ‰€æœ‰ç±»å‹çš„NaNå€¼
            nan_mask = df[col].isna()
            if nan_mask.any():
                nan_count += nan_mask.sum()
                if df[col].dtype in ['float64', 'float32', 'int64', 'int32']:
                    # å¯¹äºæ•°å€¼åˆ—ï¼Œå°†NaNæ›¿æ¢ä¸º0
                    df.loc[nan_mask, col] = 0
                else:
                    # å¯¹äºå…¶ä»–åˆ—ï¼Œå°†NaNæ›¿æ¢ä¸ºç©ºå­—ç¬¦ä¸²
                    df.loc[nan_mask, col] = ''
        
        if nan_count > 0:
            print(f"ğŸ”§ å¤„ç†NaNå€¼: {nan_count}ä¸ª")
            print(f"nan_values_handled å¤„ç†NaNå€¼: {nan_count}ä¸ª")
        
        print(f"âœ… DataFrameæ¸…ç†å®Œæˆ: {original_rows}è¡Œ â†’ {len(df)}è¡Œ, {original_cols}åˆ— â†’ {len(df.columns)}åˆ—")
        print(f"dataframe_cleaning_complete DataFrameæ¸…ç†å®Œæˆ: {original_rows}è¡Œ â†’ {len(df)}è¡Œ, {original_cols}åˆ— â†’ {len(df.columns)}åˆ—")
        
        return df


# å…¨å±€å®ä¾‹
excel_processor = ExcelProcessor()

